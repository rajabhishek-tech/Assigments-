{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PREDICT THE BURNED AREA OF FOREST FIRES WITH NEURAL NETWORKS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your first MLP in Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>...</th>\n",
       "      <th>monthfeb</th>\n",
       "      <th>monthjan</th>\n",
       "      <th>monthjul</th>\n",
       "      <th>monthjun</th>\n",
       "      <th>monthmar</th>\n",
       "      <th>monthmay</th>\n",
       "      <th>monthnov</th>\n",
       "      <th>monthoct</th>\n",
       "      <th>monthsep</th>\n",
       "      <th>size_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mar</td>\n",
       "      <td>fri</td>\n",
       "      <td>86.2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>94.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>51</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oct</td>\n",
       "      <td>tue</td>\n",
       "      <td>90.6</td>\n",
       "      <td>35.4</td>\n",
       "      <td>669.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oct</td>\n",
       "      <td>sat</td>\n",
       "      <td>90.6</td>\n",
       "      <td>43.7</td>\n",
       "      <td>686.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>33</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mar</td>\n",
       "      <td>fri</td>\n",
       "      <td>91.7</td>\n",
       "      <td>33.3</td>\n",
       "      <td>77.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>97</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mar</td>\n",
       "      <td>sun</td>\n",
       "      <td>89.3</td>\n",
       "      <td>51.3</td>\n",
       "      <td>102.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>11.4</td>\n",
       "      <td>99</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>aug</td>\n",
       "      <td>sun</td>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>27.8</td>\n",
       "      <td>32</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>aug</td>\n",
       "      <td>sun</td>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>21.9</td>\n",
       "      <td>71</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>aug</td>\n",
       "      <td>sun</td>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>21.2</td>\n",
       "      <td>70</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>aug</td>\n",
       "      <td>sat</td>\n",
       "      <td>94.4</td>\n",
       "      <td>146.0</td>\n",
       "      <td>614.7</td>\n",
       "      <td>11.3</td>\n",
       "      <td>25.6</td>\n",
       "      <td>42</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>nov</td>\n",
       "      <td>tue</td>\n",
       "      <td>79.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>106.7</td>\n",
       "      <td>1.1</td>\n",
       "      <td>11.8</td>\n",
       "      <td>31</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    month  day  FFMC    DMC     DC   ISI  temp  RH  wind  rain  ...  monthfeb  \\\n",
       "0     mar  fri  86.2   26.2   94.3   5.1   8.2  51   6.7   0.0  ...         0   \n",
       "1     oct  tue  90.6   35.4  669.1   6.7  18.0  33   0.9   0.0  ...         0   \n",
       "2     oct  sat  90.6   43.7  686.9   6.7  14.6  33   1.3   0.0  ...         0   \n",
       "3     mar  fri  91.7   33.3   77.5   9.0   8.3  97   4.0   0.2  ...         0   \n",
       "4     mar  sun  89.3   51.3  102.2   9.6  11.4  99   1.8   0.0  ...         0   \n",
       "..    ...  ...   ...    ...    ...   ...   ...  ..   ...   ...  ...       ...   \n",
       "512   aug  sun  81.6   56.7  665.6   1.9  27.8  32   2.7   0.0  ...         0   \n",
       "513   aug  sun  81.6   56.7  665.6   1.9  21.9  71   5.8   0.0  ...         0   \n",
       "514   aug  sun  81.6   56.7  665.6   1.9  21.2  70   6.7   0.0  ...         0   \n",
       "515   aug  sat  94.4  146.0  614.7  11.3  25.6  42   4.0   0.0  ...         0   \n",
       "516   nov  tue  79.5    3.0  106.7   1.1  11.8  31   4.5   0.0  ...         0   \n",
       "\n",
       "     monthjan  monthjul  monthjun  monthmar  monthmay  monthnov  monthoct  \\\n",
       "0           0         0         0         1         0         0         0   \n",
       "1           0         0         0         0         0         0         1   \n",
       "2           0         0         0         0         0         0         1   \n",
       "3           0         0         0         1         0         0         0   \n",
       "4           0         0         0         1         0         0         0   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "512         0         0         0         0         0         0         0   \n",
       "513         0         0         0         0         0         0         0   \n",
       "514         0         0         0         0         0         0         0   \n",
       "515         0         0         0         0         0         0         0   \n",
       "516         0         0         0         0         0         1         0   \n",
       "\n",
       "     monthsep  size_category  \n",
       "0           0          small  \n",
       "1           0          small  \n",
       "2           0          small  \n",
       "3           0          small  \n",
       "4           0          small  \n",
       "..        ...            ...  \n",
       "512         0          large  \n",
       "513         0          large  \n",
       "514         0          large  \n",
       "515         0          small  \n",
       "516         0          small  \n",
       "\n",
       "[517 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "dataset= pd.read_csv('forestfires.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 517 entries, 0 to 516\n",
      "Data columns (total 31 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   month          517 non-null    object \n",
      " 1   day            517 non-null    object \n",
      " 2   FFMC           517 non-null    float64\n",
      " 3   DMC            517 non-null    float64\n",
      " 4   DC             517 non-null    float64\n",
      " 5   ISI            517 non-null    float64\n",
      " 6   temp           517 non-null    float64\n",
      " 7   RH             517 non-null    int64  \n",
      " 8   wind           517 non-null    float64\n",
      " 9   rain           517 non-null    float64\n",
      " 10  area           517 non-null    float64\n",
      " 11  dayfri         517 non-null    int64  \n",
      " 12  daymon         517 non-null    int64  \n",
      " 13  daysat         517 non-null    int64  \n",
      " 14  daysun         517 non-null    int64  \n",
      " 15  daythu         517 non-null    int64  \n",
      " 16  daytue         517 non-null    int64  \n",
      " 17  daywed         517 non-null    int64  \n",
      " 18  monthapr       517 non-null    int64  \n",
      " 19  monthaug       517 non-null    int64  \n",
      " 20  monthdec       517 non-null    int64  \n",
      " 21  monthfeb       517 non-null    int64  \n",
      " 22  monthjan       517 non-null    int64  \n",
      " 23  monthjul       517 non-null    int64  \n",
      " 24  monthjun       517 non-null    int64  \n",
      " 25  monthmar       517 non-null    int64  \n",
      " 26  monthmay       517 non-null    int64  \n",
      " 27  monthnov       517 non-null    int64  \n",
      " 28  monthoct       517 non-null    int64  \n",
      " 29  monthsep       517 non-null    int64  \n",
      " 30  size_category  517 non-null    object \n",
      "dtypes: float64(8), int64(20), object(3)\n",
      "memory usage: 125.3+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>area</th>\n",
       "      <th>...</th>\n",
       "      <th>monthfeb</th>\n",
       "      <th>monthjan</th>\n",
       "      <th>monthjul</th>\n",
       "      <th>monthjun</th>\n",
       "      <th>monthmar</th>\n",
       "      <th>monthmay</th>\n",
       "      <th>monthnov</th>\n",
       "      <th>monthoct</th>\n",
       "      <th>monthsep</th>\n",
       "      <th>size_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fri</td>\n",
       "      <td>86.2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>94.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>51</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tue</td>\n",
       "      <td>90.6</td>\n",
       "      <td>35.4</td>\n",
       "      <td>669.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sat</td>\n",
       "      <td>90.6</td>\n",
       "      <td>43.7</td>\n",
       "      <td>686.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>33</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fri</td>\n",
       "      <td>91.7</td>\n",
       "      <td>33.3</td>\n",
       "      <td>77.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>97</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sun</td>\n",
       "      <td>89.3</td>\n",
       "      <td>51.3</td>\n",
       "      <td>102.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>11.4</td>\n",
       "      <td>99</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>sun</td>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>27.8</td>\n",
       "      <td>32</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.44</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>sun</td>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>21.9</td>\n",
       "      <td>71</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>sun</td>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>21.2</td>\n",
       "      <td>70</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>sat</td>\n",
       "      <td>94.4</td>\n",
       "      <td>146.0</td>\n",
       "      <td>614.7</td>\n",
       "      <td>11.3</td>\n",
       "      <td>25.6</td>\n",
       "      <td>42</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>tue</td>\n",
       "      <td>79.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>106.7</td>\n",
       "      <td>1.1</td>\n",
       "      <td>11.8</td>\n",
       "      <td>31</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     day  FFMC    DMC     DC   ISI  temp  RH  wind  rain   area  ...  \\\n",
       "0    fri  86.2   26.2   94.3   5.1   8.2  51   6.7   0.0   0.00  ...   \n",
       "1    tue  90.6   35.4  669.1   6.7  18.0  33   0.9   0.0   0.00  ...   \n",
       "2    sat  90.6   43.7  686.9   6.7  14.6  33   1.3   0.0   0.00  ...   \n",
       "3    fri  91.7   33.3   77.5   9.0   8.3  97   4.0   0.2   0.00  ...   \n",
       "4    sun  89.3   51.3  102.2   9.6  11.4  99   1.8   0.0   0.00  ...   \n",
       "..   ...   ...    ...    ...   ...   ...  ..   ...   ...    ...  ...   \n",
       "512  sun  81.6   56.7  665.6   1.9  27.8  32   2.7   0.0   6.44  ...   \n",
       "513  sun  81.6   56.7  665.6   1.9  21.9  71   5.8   0.0  54.29  ...   \n",
       "514  sun  81.6   56.7  665.6   1.9  21.2  70   6.7   0.0  11.16  ...   \n",
       "515  sat  94.4  146.0  614.7  11.3  25.6  42   4.0   0.0   0.00  ...   \n",
       "516  tue  79.5    3.0  106.7   1.1  11.8  31   4.5   0.0   0.00  ...   \n",
       "\n",
       "     monthfeb  monthjan  monthjul  monthjun  monthmar  monthmay  monthnov  \\\n",
       "0           0         0         0         0         1         0         0   \n",
       "1           0         0         0         0         0         0         0   \n",
       "2           0         0         0         0         0         0         0   \n",
       "3           0         0         0         0         1         0         0   \n",
       "4           0         0         0         0         1         0         0   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "512         0         0         0         0         0         0         0   \n",
       "513         0         0         0         0         0         0         0   \n",
       "514         0         0         0         0         0         0         0   \n",
       "515         0         0         0         0         0         0         0   \n",
       "516         0         0         0         0         0         0         1   \n",
       "\n",
       "     monthoct  monthsep  size_category  \n",
       "0           0         0          small  \n",
       "1           1         0          small  \n",
       "2           1         0          small  \n",
       "3           0         0          small  \n",
       "4           0         0          small  \n",
       "..        ...       ...            ...  \n",
       "512         0         0          large  \n",
       "513         0         0          large  \n",
       "514         0         0          large  \n",
       "515         0         0          small  \n",
       "516         0         0          small  \n",
       "\n",
       "[517 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=dataset.drop(['month'], axis=1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>area</th>\n",
       "      <th>dayfri</th>\n",
       "      <th>...</th>\n",
       "      <th>monthfeb</th>\n",
       "      <th>monthjan</th>\n",
       "      <th>monthjul</th>\n",
       "      <th>monthjun</th>\n",
       "      <th>monthmar</th>\n",
       "      <th>monthmay</th>\n",
       "      <th>monthnov</th>\n",
       "      <th>monthoct</th>\n",
       "      <th>monthsep</th>\n",
       "      <th>size_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86.2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>94.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>51</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.6</td>\n",
       "      <td>35.4</td>\n",
       "      <td>669.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.6</td>\n",
       "      <td>43.7</td>\n",
       "      <td>686.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>33</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91.7</td>\n",
       "      <td>33.3</td>\n",
       "      <td>77.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>97</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.3</td>\n",
       "      <td>51.3</td>\n",
       "      <td>102.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>11.4</td>\n",
       "      <td>99</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>27.8</td>\n",
       "      <td>32</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.44</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>21.9</td>\n",
       "      <td>71</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.29</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>21.2</td>\n",
       "      <td>70</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.16</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>94.4</td>\n",
       "      <td>146.0</td>\n",
       "      <td>614.7</td>\n",
       "      <td>11.3</td>\n",
       "      <td>25.6</td>\n",
       "      <td>42</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>79.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>106.7</td>\n",
       "      <td>1.1</td>\n",
       "      <td>11.8</td>\n",
       "      <td>31</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     FFMC    DMC     DC   ISI  temp  RH  wind  rain   area  dayfri  ...  \\\n",
       "0    86.2   26.2   94.3   5.1   8.2  51   6.7   0.0   0.00       1  ...   \n",
       "1    90.6   35.4  669.1   6.7  18.0  33   0.9   0.0   0.00       0  ...   \n",
       "2    90.6   43.7  686.9   6.7  14.6  33   1.3   0.0   0.00       0  ...   \n",
       "3    91.7   33.3   77.5   9.0   8.3  97   4.0   0.2   0.00       1  ...   \n",
       "4    89.3   51.3  102.2   9.6  11.4  99   1.8   0.0   0.00       0  ...   \n",
       "..    ...    ...    ...   ...   ...  ..   ...   ...    ...     ...  ...   \n",
       "512  81.6   56.7  665.6   1.9  27.8  32   2.7   0.0   6.44       0  ...   \n",
       "513  81.6   56.7  665.6   1.9  21.9  71   5.8   0.0  54.29       0  ...   \n",
       "514  81.6   56.7  665.6   1.9  21.2  70   6.7   0.0  11.16       0  ...   \n",
       "515  94.4  146.0  614.7  11.3  25.6  42   4.0   0.0   0.00       0  ...   \n",
       "516  79.5    3.0  106.7   1.1  11.8  31   4.5   0.0   0.00       0  ...   \n",
       "\n",
       "     monthfeb  monthjan  monthjul  monthjun  monthmar  monthmay  monthnov  \\\n",
       "0           0         0         0         0         1         0         0   \n",
       "1           0         0         0         0         0         0         0   \n",
       "2           0         0         0         0         0         0         0   \n",
       "3           0         0         0         0         1         0         0   \n",
       "4           0         0         0         0         1         0         0   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "512         0         0         0         0         0         0         0   \n",
       "513         0         0         0         0         0         0         0   \n",
       "514         0         0         0         0         0         0         0   \n",
       "515         0         0         0         0         0         0         0   \n",
       "516         0         0         0         0         0         0         1   \n",
       "\n",
       "     monthoct  monthsep  size_category  \n",
       "0           0         0          small  \n",
       "1           1         0          small  \n",
       "2           1         0          small  \n",
       "3           0         0          small  \n",
       "4           0         0          small  \n",
       "..        ...       ...            ...  \n",
       "512         0         0          large  \n",
       "513         0         0          large  \n",
       "514         0         0          large  \n",
       "515         0         0          small  \n",
       "516         0         0          small  \n",
       "\n",
       "[517 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=dataset.drop(['day'], axis=1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>area</th>\n",
       "      <th>dayfri</th>\n",
       "      <th>...</th>\n",
       "      <th>monthdec</th>\n",
       "      <th>monthfeb</th>\n",
       "      <th>monthjan</th>\n",
       "      <th>monthjul</th>\n",
       "      <th>monthjun</th>\n",
       "      <th>monthmar</th>\n",
       "      <th>monthmay</th>\n",
       "      <th>monthnov</th>\n",
       "      <th>monthoct</th>\n",
       "      <th>monthsep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>90.644681</td>\n",
       "      <td>110.872340</td>\n",
       "      <td>547.940039</td>\n",
       "      <td>9.021663</td>\n",
       "      <td>18.889168</td>\n",
       "      <td>44.288201</td>\n",
       "      <td>4.017602</td>\n",
       "      <td>0.021663</td>\n",
       "      <td>12.847292</td>\n",
       "      <td>0.164410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017408</td>\n",
       "      <td>0.038685</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>0.061896</td>\n",
       "      <td>0.032882</td>\n",
       "      <td>0.104449</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>0.001934</td>\n",
       "      <td>0.029014</td>\n",
       "      <td>0.332689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.520111</td>\n",
       "      <td>64.046482</td>\n",
       "      <td>248.066192</td>\n",
       "      <td>4.559477</td>\n",
       "      <td>5.806625</td>\n",
       "      <td>16.317469</td>\n",
       "      <td>1.791653</td>\n",
       "      <td>0.295959</td>\n",
       "      <td>63.655818</td>\n",
       "      <td>0.371006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130913</td>\n",
       "      <td>0.193029</td>\n",
       "      <td>0.062137</td>\n",
       "      <td>0.241199</td>\n",
       "      <td>0.178500</td>\n",
       "      <td>0.306138</td>\n",
       "      <td>0.062137</td>\n",
       "      <td>0.043980</td>\n",
       "      <td>0.168007</td>\n",
       "      <td>0.471632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.700000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>7.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>90.200000</td>\n",
       "      <td>68.600000</td>\n",
       "      <td>437.700000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>91.600000</td>\n",
       "      <td>108.300000</td>\n",
       "      <td>664.200000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>19.300000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>92.900000</td>\n",
       "      <td>142.400000</td>\n",
       "      <td>713.900000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>22.800000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.570000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>96.200000</td>\n",
       "      <td>291.300000</td>\n",
       "      <td>860.600000</td>\n",
       "      <td>56.100000</td>\n",
       "      <td>33.300000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>1090.840000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             FFMC         DMC          DC         ISI        temp          RH  \\\n",
       "count  517.000000  517.000000  517.000000  517.000000  517.000000  517.000000   \n",
       "mean    90.644681  110.872340  547.940039    9.021663   18.889168   44.288201   \n",
       "std      5.520111   64.046482  248.066192    4.559477    5.806625   16.317469   \n",
       "min     18.700000    1.100000    7.900000    0.000000    2.200000   15.000000   \n",
       "25%     90.200000   68.600000  437.700000    6.500000   15.500000   33.000000   \n",
       "50%     91.600000  108.300000  664.200000    8.400000   19.300000   42.000000   \n",
       "75%     92.900000  142.400000  713.900000   10.800000   22.800000   53.000000   \n",
       "max     96.200000  291.300000  860.600000   56.100000   33.300000  100.000000   \n",
       "\n",
       "             wind        rain         area      dayfri  ...    monthdec  \\\n",
       "count  517.000000  517.000000   517.000000  517.000000  ...  517.000000   \n",
       "mean     4.017602    0.021663    12.847292    0.164410  ...    0.017408   \n",
       "std      1.791653    0.295959    63.655818    0.371006  ...    0.130913   \n",
       "min      0.400000    0.000000     0.000000    0.000000  ...    0.000000   \n",
       "25%      2.700000    0.000000     0.000000    0.000000  ...    0.000000   \n",
       "50%      4.000000    0.000000     0.520000    0.000000  ...    0.000000   \n",
       "75%      4.900000    0.000000     6.570000    0.000000  ...    0.000000   \n",
       "max      9.400000    6.400000  1090.840000    1.000000  ...    1.000000   \n",
       "\n",
       "         monthfeb    monthjan    monthjul    monthjun    monthmar    monthmay  \\\n",
       "count  517.000000  517.000000  517.000000  517.000000  517.000000  517.000000   \n",
       "mean     0.038685    0.003868    0.061896    0.032882    0.104449    0.003868   \n",
       "std      0.193029    0.062137    0.241199    0.178500    0.306138    0.062137   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "75%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "         monthnov    monthoct    monthsep  \n",
       "count  517.000000  517.000000  517.000000  \n",
       "mean     0.001934    0.029014    0.332689  \n",
       "std      0.043980    0.168007    0.471632  \n",
       "min      0.000000    0.000000    0.000000  \n",
       "25%      0.000000    0.000000    0.000000  \n",
       "50%      0.000000    0.000000    0.000000  \n",
       "75%      0.000000    0.000000    1.000000  \n",
       "max      1.000000    1.000000    1.000000  \n",
       "\n",
       "[8 rows x 28 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"size_category\"].value_counts()\n",
    "dataset.isnull().sum()\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>area</th>\n",
       "      <th>dayfri</th>\n",
       "      <th>...</th>\n",
       "      <th>monthfeb</th>\n",
       "      <th>monthjan</th>\n",
       "      <th>monthjul</th>\n",
       "      <th>monthjun</th>\n",
       "      <th>monthmar</th>\n",
       "      <th>monthmay</th>\n",
       "      <th>monthnov</th>\n",
       "      <th>monthoct</th>\n",
       "      <th>monthsep</th>\n",
       "      <th>size_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86.2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>94.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>51</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.6</td>\n",
       "      <td>35.4</td>\n",
       "      <td>669.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.6</td>\n",
       "      <td>43.7</td>\n",
       "      <td>686.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>33</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91.7</td>\n",
       "      <td>33.3</td>\n",
       "      <td>77.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>97</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.3</td>\n",
       "      <td>51.3</td>\n",
       "      <td>102.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>11.4</td>\n",
       "      <td>99</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>27.8</td>\n",
       "      <td>32</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.44</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>21.9</td>\n",
       "      <td>71</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.29</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>81.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>665.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>21.2</td>\n",
       "      <td>70</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.16</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>94.4</td>\n",
       "      <td>146.0</td>\n",
       "      <td>614.7</td>\n",
       "      <td>11.3</td>\n",
       "      <td>25.6</td>\n",
       "      <td>42</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>79.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>106.7</td>\n",
       "      <td>1.1</td>\n",
       "      <td>11.8</td>\n",
       "      <td>31</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     FFMC    DMC     DC   ISI  temp  RH  wind  rain   area  dayfri  ...  \\\n",
       "0    86.2   26.2   94.3   5.1   8.2  51   6.7   0.0   0.00       1  ...   \n",
       "1    90.6   35.4  669.1   6.7  18.0  33   0.9   0.0   0.00       0  ...   \n",
       "2    90.6   43.7  686.9   6.7  14.6  33   1.3   0.0   0.00       0  ...   \n",
       "3    91.7   33.3   77.5   9.0   8.3  97   4.0   0.2   0.00       1  ...   \n",
       "4    89.3   51.3  102.2   9.6  11.4  99   1.8   0.0   0.00       0  ...   \n",
       "..    ...    ...    ...   ...   ...  ..   ...   ...    ...     ...  ...   \n",
       "512  81.6   56.7  665.6   1.9  27.8  32   2.7   0.0   6.44       0  ...   \n",
       "513  81.6   56.7  665.6   1.9  21.9  71   5.8   0.0  54.29       0  ...   \n",
       "514  81.6   56.7  665.6   1.9  21.2  70   6.7   0.0  11.16       0  ...   \n",
       "515  94.4  146.0  614.7  11.3  25.6  42   4.0   0.0   0.00       0  ...   \n",
       "516  79.5    3.0  106.7   1.1  11.8  31   4.5   0.0   0.00       0  ...   \n",
       "\n",
       "     monthfeb  monthjan  monthjul  monthjun  monthmar  monthmay  monthnov  \\\n",
       "0           0         0         0         0         1         0         0   \n",
       "1           0         0         0         0         0         0         0   \n",
       "2           0         0         0         0         0         0         0   \n",
       "3           0         0         0         0         1         0         0   \n",
       "4           0         0         0         0         1         0         0   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "512         0         0         0         0         0         0         0   \n",
       "513         0         0         0         0         0         0         0   \n",
       "514         0         0         0         0         0         0         0   \n",
       "515         0         0         0         0         0         0         0   \n",
       "516         0         0         0         0         0         0         1   \n",
       "\n",
       "     monthoct  monthsep  size_category  \n",
       "0           0         0              0  \n",
       "1           1         0              0  \n",
       "2           1         0              0  \n",
       "3           0         0              0  \n",
       "4           0         0              0  \n",
       "..        ...       ...            ...  \n",
       "512         0         0              1  \n",
       "513         0         0              1  \n",
       "514         0         0              1  \n",
       "515         0         0              0  \n",
       "516         0         0              0  \n",
       "\n",
       "[517 rows x 29 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.loc[dataset[\"size_category\"]=='small','size_category']=0\n",
    "dataset.loc[dataset[\"size_category\"]=='large','size_category']=1\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 517 entries, 0 to 516\n",
      "Data columns (total 29 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   FFMC           517 non-null    float64\n",
      " 1   DMC            517 non-null    float64\n",
      " 2   DC             517 non-null    float64\n",
      " 3   ISI            517 non-null    float64\n",
      " 4   temp           517 non-null    float64\n",
      " 5   RH             517 non-null    int64  \n",
      " 6   wind           517 non-null    float64\n",
      " 7   rain           517 non-null    float64\n",
      " 8   area           517 non-null    float64\n",
      " 9   dayfri         517 non-null    int64  \n",
      " 10  daymon         517 non-null    int64  \n",
      " 11  daysat         517 non-null    int64  \n",
      " 12  daysun         517 non-null    int64  \n",
      " 13  daythu         517 non-null    int64  \n",
      " 14  daytue         517 non-null    int64  \n",
      " 15  daywed         517 non-null    int64  \n",
      " 16  monthapr       517 non-null    int64  \n",
      " 17  monthaug       517 non-null    int64  \n",
      " 18  monthdec       517 non-null    int64  \n",
      " 19  monthfeb       517 non-null    int64  \n",
      " 20  monthjan       517 non-null    int64  \n",
      " 21  monthjul       517 non-null    int64  \n",
      " 22  monthjun       517 non-null    int64  \n",
      " 23  monthmar       517 non-null    int64  \n",
      " 24  monthmay       517 non-null    int64  \n",
      " 25  monthnov       517 non-null    int64  \n",
      " 26  monthoct       517 non-null    int64  \n",
      " 27  monthsep       517 non-null    int64  \n",
      " 28  size_category  517 non-null    object \n",
      "dtypes: float64(8), int64(20), object(1)\n",
      "memory usage: 117.3+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    378\n",
       "1    139\n",
       "Name: size_category, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"size_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(517, 29)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFMC             float64\n",
      "DMC              float64\n",
      "DC               float64\n",
      "ISI              float64\n",
      "temp             float64\n",
      "RH                 int64\n",
      "wind             float64\n",
      "rain             float64\n",
      "area             float64\n",
      "dayfri             int64\n",
      "daymon             int64\n",
      "daysat             int64\n",
      "daysun             int64\n",
      "daythu             int64\n",
      "daytue             int64\n",
      "daywed             int64\n",
      "monthapr           int64\n",
      "monthaug           int64\n",
      "monthdec           int64\n",
      "monthfeb           int64\n",
      "monthjan           int64\n",
      "monthjul           int64\n",
      "monthjun           int64\n",
      "monthmar           int64\n",
      "monthmay           int64\n",
      "monthnov           int64\n",
      "monthoct           int64\n",
      "monthsep           int64\n",
      "size_category      int32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "dataset[\"size_category\"] = dataset[\"size_category\"].astype(str).astype(int)\n",
    "print(dataset.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=dataset.iloc[:,:28]\n",
    "Y=dataset['size_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAATgUlEQVR4nO3dbYye5Zne8f8RIGzKREBKduQ1bO12nXZ5adhlSqOmrWbCqjj0gxNpUzlFWdggOVXZKqvmQyAfmqyQpURaNlUh7NZZItxCM7VIUlMStmLpTmm0oSyOCMYQGje4rCGylRhMhiIqO2c/zE131h57Hj9vw1zz/0mj57lfr/McW8fcvnw/96SqkCS15W0rXYAkafgMd0lqkOEuSQ0y3CWpQYa7JDXo7JUuAOCiiy6qDRs29H38a6+9xnnnnTe8gt7i1lq/YM9rhT2fmT179vy4qt691La3RLhv2LCBJ554ou/j5+bmmJ6eHl5Bb3FrrV+w57XCns9Mkv99qm1Oy0hSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoPeEp9QHdTeF49y4y3fHPu4Bz7/j8c+piT1wit3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQcuGe5KfS/J4ku8l2Zfkd7r1n0vyYpInu6/rFh1za5L9SZ5Lcu0oG5AknayXB4e9AXygquaTnAN8O8lD3bYvVtXvLt45yaXAVuAy4BeAP07ynqo6PszCJUmntuyVey2Y7xbP6b7qNIdsAWar6o2qeh7YD1w9cKWSpJ6l6nQ53e2UnAXsAX4J+FJVfTrJ54AbgVeBJ4BPVdXLSe4EHquqe7tj7wYeqqr7TzjnNmAbwOTk5FWzs7N9N3H4yFEOvd734X27Yv354x8UmJ+fZ2JiYkXGXin2vDbY85mZmZnZU1VTS23r6Xnu3ZTKlUkuAL6R5HLg94HbWLiKvw24Hfg4kKVOscQ5dwA7AKampmp6erqXUpZ0x327uX3v+B9Nf+D66bGPCTA3N8cg36/VyJ7XBnsenjO6W6aqXgHmgM1VdaiqjlfVz4Av8xdTLweBSxYddjHw0uClSpJ61cvdMu/urthJ8g7g14DvJ1m3aLcPA0937x8AtiY5N8lGYBPw+FCrliSdVi9zGeuAnd28+9uAXVX1YJJ/n+RKFqZcDgCfAKiqfUl2Ac8Ax4CbvVNGksZr2XCvqqeAX1li/cdOc8x2YPtgpUmS+uUnVCWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNaiXX5D9c0keT/K9JPuS/E63/l1JHk7yg+71wkXH3Jpkf5Lnklw7ygYkSSfr5cr9DeADVfVe4Epgc5L3AbcAj1TVJuCRbpkklwJbgcuAzcBd3S/XliSNybLhXgvmu8Vzuq8CtgA7u/U7gQ9177cAs1X1RlU9D+wHrh5m0ZKk00tVLb/TwpX3HuCXgC9V1aeTvFJVFyza5+WqujDJncBjVXVvt/5u4KGquv+Ec24DtgFMTk5eNTs723cTh48c5dDrfR/etyvWnz/+QYH5+XkmJiZWZOyVYs9rgz2fmZmZmT1VNbXUtrN7OUFVHQeuTHIB8I0kl59m9yx1iiXOuQPYATA1NVXT09O9lLKkO+7bze17e2plqA5cPz32MQHm5uYY5Pu1Gtnz2mDPw3NGd8tU1SvAHAtz6YeSrAPoXg93ux0ELll02MXAS4MWKknqXS93y7y7u2InyTuAXwO+DzwA3NDtdgOwu3v/ALA1yblJNgKbgMeHXLck6TR6mctYB+zs5t3fBuyqqgeTfAfYleQm4AXgIwBVtS/JLuAZ4BhwczetI0kak2XDvaqeAn5lifU/Aa45xTHbge0DVydJ6oufUJWkBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIatGy4J7kkyZ8keTbJviSf7NZ/LsmLSZ7svq5bdMytSfYneS7JtaNsQJJ0smV/QTZwDPhUVX03yTuBPUke7rZ9sap+d/HOSS4FtgKXAb8A/HGS91TV8WEWLkk6tWWv3KvqR1X13e79T4FngfWnOWQLMFtVb1TV88B+4OphFCtJ6k2qqvedkw3Ao8DlwL8EbgReBZ5g4er+5SR3Ao9V1b3dMXcDD1XV/SecaxuwDWBycvKq2dnZvps4fOQoh17v+/C+XbH+/PEPCszPzzMxMbEiY68Ue14b7PnMzMzM7KmqqaW29TItA0CSCeBrwG9X1atJfh+4Daju9Xbg40CWOPyknyBVtQPYATA1NVXT09O9lnKSO+7bze17e25laA5cPz32MQHm5uYY5Pu1Gtnz2mDPw9PT3TJJzmEh2O+rqq8DVNWhqjpeVT8DvsxfTL0cBC5ZdPjFwEvDK1mStJxe7pYJcDfwbFX93qL16xbt9mHg6e79A8DWJOcm2QhsAh4fXsmSpOX0MpfxfuBjwN4kT3brPgN8NMmVLEy5HAA+AVBV+5LsAp5h4U6bm71TRpLGa9lwr6pvs/Q8+rdOc8x2YPsAdUmSBuAnVCWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNaiXX5B9SZI/SfJskn1JPtmtf1eSh5P8oHu9cNExtybZn+S5JNeOsgFJ0sl6uXI/Bnyqqn4ZeB9wc5JLgVuAR6pqE/BIt0y3bStwGbAZuCvJWaMoXpK0tGXDvap+VFXf7d7/FHgWWA9sAXZ2u+0EPtS93wLMVtUbVfU8sB+4esh1S5JOI1XV+87JBuBR4HLghaq6YNG2l6vqwiR3Ao9V1b3d+ruBh6rq/hPOtQ3YBjA5OXnV7Oxs300cPnKUQ6/3fXjfrlh//vgHBebn55mYmFiRsVeKPa8N9nxmZmZm9lTV1FLbzu71JEkmgK8Bv11VryY55a5LrDvpJ0hV7QB2AExNTdX09HSvpZzkjvt2c/venlsZmgPXT499TIC5uTkG+X6tRva8Ntjz8PR0t0ySc1gI9vuq6uvd6kNJ1nXb1wGHu/UHgUsWHX4x8NJwypUk9aKXu2UC3A08W1W/t2jTA8AN3fsbgN2L1m9Ncm6SjcAm4PHhlSxJWk4vcxnvBz4G7E3yZLfuM8DngV1JbgJeAD4CUFX7kuwCnmHhTpubq+r4sAuXJJ3asuFeVd9m6Xl0gGtOccx2YPsAdUmSBuAnVCWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNaiXX5D9lSSHkzy9aN3nkryY5Mnu67pF225Nsj/Jc0muHVXhkqRT6+XK/R5g8xLrv1hVV3Zf3wJIcimwFbisO+auJGcNq1hJUm+WDfeqehQ40uP5tgCzVfVGVT0P7AeuHqA+SVIfBplz/60kT3XTNhd269YDf75on4PdOknSGKWqlt8p2QA8WFWXd8uTwI+BAm4D1lXVx5N8CfhOVd3b7Xc38K2q+toS59wGbAOYnJy8anZ2tu8mDh85yqHX+z68b1esP3/8gwLz8/NMTEysyNgrxZ7XBns+MzMzM3uqamqpbWf3c8KqOvTm+yRfBh7sFg8Clyza9WLgpVOcYwewA2Bqaqqmp6f7KQWAO+7bze17+2plIAeunx77mABzc3MM8v1ajex5bbDn4elrWibJukWLHwbevJPmAWBrknOTbAQ2AY8PVqIk6Uwte7mb5KvANHBRkoPAZ4HpJFeyMC1zAPgEQFXtS7ILeAY4BtxcVcdHUrkk6ZSWDfeq+ugSq+8+zf7bge2DFCVJGoyfUJWkBo3/fyEl6S1mwy3fXLGx79l83kjO65W7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNWjZcE/ylSSHkzy9aN27kjyc5Afd64WLtt2aZH+S55JcO6rCJUmn1suV+z3A5hPW3QI8UlWbgEe6ZZJcCmwFLuuOuSvJWUOrVpLUk2XDvaoeBY6csHoLsLN7vxP40KL1s1X1RlU9D+wHrh5OqZKkXqWqlt8p2QA8WFWXd8uvVNUFi7a/XFUXJrkTeKyq7u3W3w08VFX3L3HObcA2gMnJyatmZ2f7buLwkaMcer3vw/t2xfrzxz8oMD8/z8TExIqMvVLseW1YqZ73vnh07GO+aeP5Z/Xd88zMzJ6qmlpq29kDVXWyLLFuyZ8eVbUD2AEwNTVV09PTfQ96x327uX3vsFtZ3oHrp8c+JsDc3ByDfL9WI3teG1aq5xtv+ebYx3zTPZvPG0nP/d4tcyjJOoDu9XC3/iBwyaL9LgZe6r88SVI/+g33B4Abuvc3ALsXrd+a5NwkG4FNwOODlShJOlPLzmUk+SowDVyU5CDwWeDzwK4kNwEvAB8BqKp9SXYBzwDHgJur6viIapckncKy4V5VHz3FpmtOsf92YPsgRUmSBuMnVCWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNWjZX7N3OkkOAD8FjgPHqmoqybuA/whsAA4A/6SqXh6sTEnSmRjGlftMVV1ZVVPd8i3AI1W1CXikW5YkjdEopmW2ADu79zuBD41gDEnSaaSq+j84eR54GSjg31bVjiSvVNUFi/Z5uaouXOLYbcA2gMnJyatmZ2f7ruPwkaMcer3vw/t2xfrzxz8oMD8/z8TExIqMvVLseW1YqZ73vnh07GO+aeP5Z/Xd88zMzJ5FsyZ/yUBz7sD7q+qlJD8PPJzk+70eWFU7gB0AU1NTNT093XcRd9y3m9v3DtrKmTtw/fTYxwSYm5tjkO/XamTPa8NK9XzjLd8c+5hvumfzeSPpeaBpmap6qXs9DHwDuBo4lGQdQPd6eNAiJUlnpu9wT3Jekne++R74R8DTwAPADd1uNwC7By1SknRmBpnLmAS+keTN8/yHqvqjJH8G7EpyE/AC8JHBy5QknYm+w72qfgi8d4n1PwGuGaQoSdJg/ISqJDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1KCRhXuSzUmeS7I/yS2jGkeSdLKRhHuSs4AvAR8ELgU+muTSUYwlSTrZqK7crwb2V9UPq+r/ArPAlhGNJUk6wdkjOu964M8XLR8E/u7iHZJsA7Z1i/NJnhtgvIuAHw9wfF/yhXGP+P+tSL8rzJ7XhjXX88wXBur5r51qw6jCPUusq7+0ULUD2DGUwZInqmpqGOdaDdZav2DPa4U9D8+opmUOApcsWr4YeGlEY0mSTjCqcP8zYFOSjUneDmwFHhjRWJKkE4xkWqaqjiX5LeC/AGcBX6mqfaMYqzOU6Z1VZK31C/a8VtjzkKSqlt9LkrSq+AlVSWqQ4S5JDVo14b7c4wyy4N90259K8qsrUecw9dDz9V2vTyX50yTvXYk6h6nXx1Yk+TtJjif59XHWNwq99JxkOsmTSfYl+W/jrnHYevi7fX6S/5zke13Pv7kSdQ5Lkq8kOZzk6VNsH35+VdVb/ouF/5T9X8BfB94OfA+49IR9rgMeYuEe+/cB/2Ol6x5Dz38PuLB7/8G10POi/f4r8C3g11e67jH8OV8APAP8Yrf88ytd9xh6/gzwhe79u4EjwNtXuvYBev6HwK8CT59i+9Dza7VcuffyOIMtwL+rBY8BFyRZN+5Ch2jZnqvqT6vq5W7xMRY+T7Ca9frYin8BfA04PM7iRqSXnv8p8PWqegGgqlZ73730XMA7kwSYYCHcj423zOGpqkdZ6OFUhp5fqyXcl3qcwfo+9llNzrSfm1j4yb+aLdtzkvXAh4E/GGNdo9TLn/N7gAuTzCXZk+Q3xlbdaPTS853AL7Pw4ce9wCer6mfjKW9FDD2/RvX4gWFb9nEGPe6zmvTcT5IZFsL974+0otHrped/DXy6qo4vXNSter30fDZwFXAN8A7gO0keq6r/OeriRqSXnq8FngQ+APwN4OEk/72qXh1xbStl6Pm1WsK9l8cZtPbIg576SfK3gT8EPlhVPxlTbaPSS89TwGwX7BcB1yU5VlX/aSwVDl+vf7d/XFWvAa8leRR4L7Baw72Xnn8T+HwtTEjvT/I88LeAx8dT4tgNPb9Wy7RML48zeAD4je5/nd8HHK2qH4270CFatuckvwh8HfjYKr6KW2zZnqtqY1VtqKoNwP3AP1/FwQ69/d3eDfyDJGcn+SssPGH12THXOUy99PwCC/9SIckk8DeBH461yvEaen6tiiv3OsXjDJL8s277H7Bw58R1wH7g/7Dwk3/V6rHnfwX8VeCu7kr2WK3iJ+r12HNTeum5qp5N8kfAU8DPgD+sqiVvqVsNevxzvg24J8leFqYsPl1Vq/ZRwEm+CkwDFyU5CHwWOAdGl18+fkCSGrRapmUkSWfAcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkN+n+Jizj4rywO8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'FFMC'}>,\n",
       "        <AxesSubplot:title={'center':'DMC'}>,\n",
       "        <AxesSubplot:title={'center':'DC'}>,\n",
       "        <AxesSubplot:title={'center':'ISI'}>,\n",
       "        <AxesSubplot:title={'center':'temp'}>],\n",
       "       [<AxesSubplot:title={'center':'RH'}>,\n",
       "        <AxesSubplot:title={'center':'wind'}>,\n",
       "        <AxesSubplot:title={'center':'rain'}>,\n",
       "        <AxesSubplot:title={'center':'area'}>,\n",
       "        <AxesSubplot:title={'center':'dayfri'}>],\n",
       "       [<AxesSubplot:title={'center':'daymon'}>,\n",
       "        <AxesSubplot:title={'center':'daysat'}>,\n",
       "        <AxesSubplot:title={'center':'daysun'}>,\n",
       "        <AxesSubplot:title={'center':'daythu'}>,\n",
       "        <AxesSubplot:title={'center':'daytue'}>],\n",
       "       [<AxesSubplot:title={'center':'daywed'}>,\n",
       "        <AxesSubplot:title={'center':'monthapr'}>,\n",
       "        <AxesSubplot:title={'center':'monthaug'}>,\n",
       "        <AxesSubplot:title={'center':'monthdec'}>,\n",
       "        <AxesSubplot:title={'center':'monthfeb'}>],\n",
       "       [<AxesSubplot:title={'center':'monthjan'}>,\n",
       "        <AxesSubplot:title={'center':'monthjul'}>,\n",
       "        <AxesSubplot:title={'center':'monthjun'}>,\n",
       "        <AxesSubplot:title={'center':'monthmar'}>,\n",
       "        <AxesSubplot:title={'center':'monthmay'}>],\n",
       "       [<AxesSubplot:title={'center':'monthnov'}>,\n",
       "        <AxesSubplot:title={'center':'monthoct'}>,\n",
       "        <AxesSubplot:title={'center':'monthsep'}>, <AxesSubplot:>,\n",
       "        <AxesSubplot:>]], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "a = StandardScaler()\n",
    "a.fit(X)\n",
    "X_standardized = a.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "      <td>5.170000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-1.754024e-15</td>\n",
       "      <td>3.070830e-16</td>\n",
       "      <td>7.387171e-17</td>\n",
       "      <td>-3.865380e-17</td>\n",
       "      <td>2.005703e-16</td>\n",
       "      <td>3.362881e-16</td>\n",
       "      <td>-2.676776e-16</td>\n",
       "      <td>-2.841054e-16</td>\n",
       "      <td>-1.274502e-16</td>\n",
       "      <td>4.874674e-17</td>\n",
       "      <td>...</td>\n",
       "      <td>7.179943e-16</td>\n",
       "      <td>-1.933764e-16</td>\n",
       "      <td>-2.260174e-17</td>\n",
       "      <td>1.352883e-17</td>\n",
       "      <td>1.169277e-16</td>\n",
       "      <td>2.265542e-16</td>\n",
       "      <td>-2.596515e-16</td>\n",
       "      <td>1.443075e-16</td>\n",
       "      <td>6.253326e-16</td>\n",
       "      <td>4.024290e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "      <td>1.000969e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.304582e+01</td>\n",
       "      <td>-1.715608e+00</td>\n",
       "      <td>-2.179108e+00</td>\n",
       "      <td>-1.980578e+00</td>\n",
       "      <td>-2.876943e+00</td>\n",
       "      <td>-1.796637e+00</td>\n",
       "      <td>-2.021098e+00</td>\n",
       "      <td>-7.326831e-02</td>\n",
       "      <td>-2.020198e-01</td>\n",
       "      <td>-4.435755e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.331035e-01</td>\n",
       "      <td>-2.006027e-01</td>\n",
       "      <td>-6.231770e-02</td>\n",
       "      <td>-2.568645e-01</td>\n",
       "      <td>-1.843909e-01</td>\n",
       "      <td>-3.415123e-01</td>\n",
       "      <td>-6.231770e-02</td>\n",
       "      <td>-4.402255e-02</td>\n",
       "      <td>-1.728597e-01</td>\n",
       "      <td>-7.060812e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-8.063453e-02</td>\n",
       "      <td>-6.606652e-01</td>\n",
       "      <td>-4.448281e-01</td>\n",
       "      <td>-5.535954e-01</td>\n",
       "      <td>-5.842379e-01</td>\n",
       "      <td>-6.924563e-01</td>\n",
       "      <td>-7.361236e-01</td>\n",
       "      <td>-7.326831e-02</td>\n",
       "      <td>-2.020198e-01</td>\n",
       "      <td>-4.435755e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.331035e-01</td>\n",
       "      <td>-2.006027e-01</td>\n",
       "      <td>-6.231770e-02</td>\n",
       "      <td>-2.568645e-01</td>\n",
       "      <td>-1.843909e-01</td>\n",
       "      <td>-3.415123e-01</td>\n",
       "      <td>-6.231770e-02</td>\n",
       "      <td>-4.402255e-02</td>\n",
       "      <td>-1.728597e-01</td>\n",
       "      <td>-7.060812e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.732292e-01</td>\n",
       "      <td>-4.020255e-02</td>\n",
       "      <td>4.691190e-01</td>\n",
       "      <td>-1.364774e-01</td>\n",
       "      <td>7.082076e-02</td>\n",
       "      <td>-1.403660e-01</td>\n",
       "      <td>-9.833712e-03</td>\n",
       "      <td>-7.326831e-02</td>\n",
       "      <td>-1.938429e-01</td>\n",
       "      <td>-4.435755e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.331035e-01</td>\n",
       "      <td>-2.006027e-01</td>\n",
       "      <td>-6.231770e-02</td>\n",
       "      <td>-2.568645e-01</td>\n",
       "      <td>-1.843909e-01</td>\n",
       "      <td>-3.415123e-01</td>\n",
       "      <td>-6.231770e-02</td>\n",
       "      <td>-4.402255e-02</td>\n",
       "      <td>-1.728597e-01</td>\n",
       "      <td>-7.060812e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.089598e-01</td>\n",
       "      <td>4.927389e-01</td>\n",
       "      <td>6.696628e-01</td>\n",
       "      <td>3.904086e-01</td>\n",
       "      <td>6.741643e-01</td>\n",
       "      <td>5.344111e-01</td>\n",
       "      <td>4.929823e-01</td>\n",
       "      <td>-7.326831e-02</td>\n",
       "      <td>-9.870852e-02</td>\n",
       "      <td>-4.435755e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.331035e-01</td>\n",
       "      <td>-2.006027e-01</td>\n",
       "      <td>-6.231770e-02</td>\n",
       "      <td>-2.568645e-01</td>\n",
       "      <td>-1.843909e-01</td>\n",
       "      <td>-3.415123e-01</td>\n",
       "      <td>-6.231770e-02</td>\n",
       "      <td>-4.402255e-02</td>\n",
       "      <td>-1.728597e-01</td>\n",
       "      <td>1.416268e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.007353e+00</td>\n",
       "      <td>2.819865e+00</td>\n",
       "      <td>1.261610e+00</td>\n",
       "      <td>1.033538e+01</td>\n",
       "      <td>2.484195e+00</td>\n",
       "      <td>3.417549e+00</td>\n",
       "      <td>3.007063e+00</td>\n",
       "      <td>2.157228e+01</td>\n",
       "      <td>1.695111e+01</td>\n",
       "      <td>2.254407e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.512952e+00</td>\n",
       "      <td>4.984977e+00</td>\n",
       "      <td>1.604681e+01</td>\n",
       "      <td>3.893103e+00</td>\n",
       "      <td>5.423261e+00</td>\n",
       "      <td>2.928152e+00</td>\n",
       "      <td>1.604681e+01</td>\n",
       "      <td>2.271563e+01</td>\n",
       "      <td>5.785038e+00</td>\n",
       "      <td>1.416268e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0             1             2             3             4   \\\n",
       "count  5.170000e+02  5.170000e+02  5.170000e+02  5.170000e+02  5.170000e+02   \n",
       "mean  -1.754024e-15  3.070830e-16  7.387171e-17 -3.865380e-17  2.005703e-16   \n",
       "std    1.000969e+00  1.000969e+00  1.000969e+00  1.000969e+00  1.000969e+00   \n",
       "min   -1.304582e+01 -1.715608e+00 -2.179108e+00 -1.980578e+00 -2.876943e+00   \n",
       "25%   -8.063453e-02 -6.606652e-01 -4.448281e-01 -5.535954e-01 -5.842379e-01   \n",
       "50%    1.732292e-01 -4.020255e-02  4.691190e-01 -1.364774e-01  7.082076e-02   \n",
       "75%    4.089598e-01  4.927389e-01  6.696628e-01  3.904086e-01  6.741643e-01   \n",
       "max    1.007353e+00  2.819865e+00  1.261610e+00  1.033538e+01  2.484195e+00   \n",
       "\n",
       "                 5             6             7             8             9   \\\n",
       "count  5.170000e+02  5.170000e+02  5.170000e+02  5.170000e+02  5.170000e+02   \n",
       "mean   3.362881e-16 -2.676776e-16 -2.841054e-16 -1.274502e-16  4.874674e-17   \n",
       "std    1.000969e+00  1.000969e+00  1.000969e+00  1.000969e+00  1.000969e+00   \n",
       "min   -1.796637e+00 -2.021098e+00 -7.326831e-02 -2.020198e-01 -4.435755e-01   \n",
       "25%   -6.924563e-01 -7.361236e-01 -7.326831e-02 -2.020198e-01 -4.435755e-01   \n",
       "50%   -1.403660e-01 -9.833712e-03 -7.326831e-02 -1.938429e-01 -4.435755e-01   \n",
       "75%    5.344111e-01  4.929823e-01 -7.326831e-02 -9.870852e-02 -4.435755e-01   \n",
       "max    3.417549e+00  3.007063e+00  2.157228e+01  1.695111e+01  2.254407e+00   \n",
       "\n",
       "       ...            18            19            20            21  \\\n",
       "count  ...  5.170000e+02  5.170000e+02  5.170000e+02  5.170000e+02   \n",
       "mean   ...  7.179943e-16 -1.933764e-16 -2.260174e-17  1.352883e-17   \n",
       "std    ...  1.000969e+00  1.000969e+00  1.000969e+00  1.000969e+00   \n",
       "min    ... -1.331035e-01 -2.006027e-01 -6.231770e-02 -2.568645e-01   \n",
       "25%    ... -1.331035e-01 -2.006027e-01 -6.231770e-02 -2.568645e-01   \n",
       "50%    ... -1.331035e-01 -2.006027e-01 -6.231770e-02 -2.568645e-01   \n",
       "75%    ... -1.331035e-01 -2.006027e-01 -6.231770e-02 -2.568645e-01   \n",
       "max    ...  7.512952e+00  4.984977e+00  1.604681e+01  3.893103e+00   \n",
       "\n",
       "                 22            23            24            25            26  \\\n",
       "count  5.170000e+02  5.170000e+02  5.170000e+02  5.170000e+02  5.170000e+02   \n",
       "mean   1.169277e-16  2.265542e-16 -2.596515e-16  1.443075e-16  6.253326e-16   \n",
       "std    1.000969e+00  1.000969e+00  1.000969e+00  1.000969e+00  1.000969e+00   \n",
       "min   -1.843909e-01 -3.415123e-01 -6.231770e-02 -4.402255e-02 -1.728597e-01   \n",
       "25%   -1.843909e-01 -3.415123e-01 -6.231770e-02 -4.402255e-02 -1.728597e-01   \n",
       "50%   -1.843909e-01 -3.415123e-01 -6.231770e-02 -4.402255e-02 -1.728597e-01   \n",
       "75%   -1.843909e-01 -3.415123e-01 -6.231770e-02 -4.402255e-02 -1.728597e-01   \n",
       "max    5.423261e+00  2.928152e+00  1.604681e+01  2.271563e+01  5.785038e+00   \n",
       "\n",
       "                 27  \n",
       "count  5.170000e+02  \n",
       "mean   4.024290e-16  \n",
       "std    1.000969e+00  \n",
       "min   -7.060812e-01  \n",
       "25%   -7.060812e-01  \n",
       "50%   -7.060812e-01  \n",
       "75%    1.416268e+00  \n",
       "max    1.416268e+00  \n",
       "\n",
       "[8 rows x 28 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_standardized).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary packages\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=28, kernel_initializer='uniform', activation='relu'))\n",
    "    model.add(Dense(28, kernel_initializer='uniform', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    \n",
    "    adam=Adam(lr=0.01)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[CV 1/5; 1/9] START batch_size=10, epochs=10....................................\n",
      "[CV 1/5; 1/9] END ..................batch_size=10, epochs=10; total time=   1.6s\n",
      "[CV 2/5; 1/9] START batch_size=10, epochs=10....................................\n",
      "[CV 2/5; 1/9] END ..................batch_size=10, epochs=10; total time=   1.5s\n",
      "[CV 3/5; 1/9] START batch_size=10, epochs=10....................................\n",
      "[CV 3/5; 1/9] END ..................batch_size=10, epochs=10; total time=   1.8s\n",
      "[CV 4/5; 1/9] START batch_size=10, epochs=10....................................\n",
      "[CV 4/5; 1/9] END ..................batch_size=10, epochs=10; total time=   1.3s\n",
      "[CV 5/5; 1/9] START batch_size=10, epochs=10....................................\n",
      "[CV 5/5; 1/9] END ..................batch_size=10, epochs=10; total time=   2.4s\n",
      "[CV 1/5; 2/9] START batch_size=10, epochs=50....................................\n",
      "[CV 1/5; 2/9] END ..................batch_size=10, epochs=50; total time=   5.3s\n",
      "[CV 2/5; 2/9] START batch_size=10, epochs=50....................................\n",
      "[CV 2/5; 2/9] END ..................batch_size=10, epochs=50; total time=   3.1s\n",
      "[CV 3/5; 2/9] START batch_size=10, epochs=50....................................\n",
      "[CV 3/5; 2/9] END ..................batch_size=10, epochs=50; total time=   2.8s\n",
      "[CV 4/5; 2/9] START batch_size=10, epochs=50....................................\n",
      "[CV 4/5; 2/9] END ..................batch_size=10, epochs=50; total time=   3.1s\n",
      "[CV 5/5; 2/9] START batch_size=10, epochs=50....................................\n",
      "[CV 5/5; 2/9] END ..................batch_size=10, epochs=50; total time=   3.1s\n",
      "[CV 1/5; 3/9] START batch_size=10, epochs=100...................................\n",
      "[CV 1/5; 3/9] END .................batch_size=10, epochs=100; total time=   4.6s\n",
      "[CV 2/5; 3/9] START batch_size=10, epochs=100...................................\n",
      "[CV 2/5; 3/9] END .................batch_size=10, epochs=100; total time=   4.5s\n",
      "[CV 3/5; 3/9] START batch_size=10, epochs=100...................................\n",
      "[CV 3/5; 3/9] END .................batch_size=10, epochs=100; total time=   4.6s\n",
      "[CV 4/5; 3/9] START batch_size=10, epochs=100...................................\n",
      "[CV 4/5; 3/9] END .................batch_size=10, epochs=100; total time=   4.9s\n",
      "[CV 5/5; 3/9] START batch_size=10, epochs=100...................................\n",
      "[CV 5/5; 3/9] END .................batch_size=10, epochs=100; total time=   4.9s\n",
      "[CV 1/5; 4/9] START batch_size=20, epochs=10....................................\n",
      "[CV 1/5; 4/9] END ..................batch_size=20, epochs=10; total time=   1.7s\n",
      "[CV 2/5; 4/9] START batch_size=20, epochs=10....................................\n",
      "[CV 2/5; 4/9] END ..................batch_size=20, epochs=10; total time=   1.0s\n",
      "[CV 3/5; 4/9] START batch_size=20, epochs=10....................................\n",
      "[CV 3/5; 4/9] END ..................batch_size=20, epochs=10; total time=   1.5s\n",
      "[CV 4/5; 4/9] START batch_size=20, epochs=10....................................\n",
      "[CV 4/5; 4/9] END ..................batch_size=20, epochs=10; total time=   1.6s\n",
      "[CV 5/5; 4/9] START batch_size=20, epochs=10....................................\n",
      "[CV 5/5; 4/9] END ..................batch_size=20, epochs=10; total time=   1.3s\n",
      "[CV 1/5; 5/9] START batch_size=20, epochs=50....................................\n",
      "[CV 1/5; 5/9] END ..................batch_size=20, epochs=50; total time=   2.4s\n",
      "[CV 2/5; 5/9] START batch_size=20, epochs=50....................................\n",
      "[CV 2/5; 5/9] END ..................batch_size=20, epochs=50; total time=   2.0s\n",
      "[CV 3/5; 5/9] START batch_size=20, epochs=50....................................\n",
      "[CV 3/5; 5/9] END ..................batch_size=20, epochs=50; total time=   2.3s\n",
      "[CV 4/5; 5/9] START batch_size=20, epochs=50....................................\n",
      "[CV 4/5; 5/9] END ..................batch_size=20, epochs=50; total time=   2.2s\n",
      "[CV 5/5; 5/9] START batch_size=20, epochs=50....................................\n",
      "[CV 5/5; 5/9] END ..................batch_size=20, epochs=50; total time=   3.2s\n",
      "[CV 1/5; 6/9] START batch_size=20, epochs=100...................................\n",
      "[CV 1/5; 6/9] END .................batch_size=20, epochs=100; total time=   5.0s\n",
      "[CV 2/5; 6/9] START batch_size=20, epochs=100...................................\n",
      "[CV 2/5; 6/9] END .................batch_size=20, epochs=100; total time=   2.8s\n",
      "[CV 3/5; 6/9] START batch_size=20, epochs=100...................................\n",
      "[CV 3/5; 6/9] END .................batch_size=20, epochs=100; total time=   2.7s\n",
      "[CV 4/5; 6/9] START batch_size=20, epochs=100...................................\n",
      "[CV 4/5; 6/9] END .................batch_size=20, epochs=100; total time=   3.3s\n",
      "[CV 5/5; 6/9] START batch_size=20, epochs=100...................................\n",
      "[CV 5/5; 6/9] END .................batch_size=20, epochs=100; total time=   3.1s\n",
      "[CV 1/5; 7/9] START batch_size=40, epochs=10....................................\n",
      "[CV 1/5; 7/9] END ..................batch_size=40, epochs=10; total time=   1.3s\n",
      "[CV 2/5; 7/9] START batch_size=40, epochs=10....................................\n",
      "[CV 2/5; 7/9] END ..................batch_size=40, epochs=10; total time=   1.7s\n",
      "[CV 3/5; 7/9] START batch_size=40, epochs=10....................................\n",
      "[CV 3/5; 7/9] END ..................batch_size=40, epochs=10; total time=   0.9s\n",
      "[CV 4/5; 7/9] START batch_size=40, epochs=10....................................\n",
      "WARNING:tensorflow:5 out of the last 16 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021885B63CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 4/5; 7/9] END ..................batch_size=40, epochs=10; total time=   1.4s\n",
      "[CV 5/5; 7/9] START batch_size=40, epochs=10....................................\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218831B68B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 7/9] END ..................batch_size=40, epochs=10; total time=   1.5s\n",
      "[CV 1/5; 8/9] START batch_size=40, epochs=50....................................\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218871548B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 8/9] END ..................batch_size=40, epochs=50; total time=   1.4s\n",
      "[CV 2/5; 8/9] START batch_size=40, epochs=50....................................\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882E5F820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 8/9] END ..................batch_size=40, epochs=50; total time=   1.8s\n",
      "[CV 3/5; 8/9] START batch_size=40, epochs=50....................................\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882FEEC10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 3/5; 8/9] END ..................batch_size=40, epochs=50; total time=   1.6s\n",
      "[CV 4/5; 8/9] START batch_size=40, epochs=50....................................\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x000002188464C8B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 4/5; 8/9] END ..................batch_size=40, epochs=50; total time=   1.9s\n",
      "[CV 5/5; 8/9] START batch_size=40, epochs=50....................................\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021887154F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 8/9] END ..................batch_size=40, epochs=50; total time=   1.2s\n",
      "[CV 1/5; 9/9] START batch_size=40, epochs=100...................................\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218857C1C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 9/9] END .................batch_size=40, epochs=100; total time=   2.7s\n",
      "[CV 2/5; 9/9] START batch_size=40, epochs=100...................................\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218831B6AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 2/5; 9/9] END .................batch_size=40, epochs=100; total time=   2.0s\n",
      "[CV 3/5; 9/9] START batch_size=40, epochs=100...................................\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218831CDAF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 3/5; 9/9] END .................batch_size=40, epochs=100; total time=   2.2s\n",
      "[CV 4/5; 9/9] START batch_size=40, epochs=100...................................\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882D89A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 4/5; 9/9] END .................batch_size=40, epochs=100; total time=   1.9s\n",
      "[CV 5/5; 9/9] START batch_size=40, epochs=100...................................\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218831B69D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 9/9] END .................batch_size=40, epochs=100; total time=   2.4s\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = KerasClassifier(build_fn = create_model,verbose = 0)\n",
    "# Define the grid search parameters\n",
    "batch_size = [10,20,40]\n",
    "epochs = [10,50,100]\n",
    "# Make a dictionary of the grid search parameters\n",
    "param_grid = dict(batch_size = batch_size,epochs = epochs)\n",
    "# Build and fit the GridSearchCV\n",
    "grid = GridSearchCV(estimator = model,param_grid = param_grid,cv = KFold(),verbose = 10)\n",
    "grid_result = grid.fit(X_standardized,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best : 0.9225541353225708, using {'batch_size': 20, 'epochs': 100}\n",
      "0.8954256892204284,0.05519570479452208 with: {'batch_size': 10, 'epochs': 10}\n",
      "0.9071321845054626,0.05633013928505657 with: {'batch_size': 10, 'epochs': 50}\n",
      "0.9128827571868896,0.04649427631529634 with: {'batch_size': 10, 'epochs': 100}\n",
      "0.8605862617492676,0.06772954317462998 with: {'batch_size': 20, 'epochs': 10}\n",
      "0.908999252319336,0.047160092218081075 with: {'batch_size': 20, 'epochs': 50}\n",
      "0.9225541353225708,0.040981364866094605 with: {'batch_size': 20, 'epochs': 100}\n",
      "0.843203890323639,0.07715694113758179 with: {'batch_size': 40, 'epochs': 10}\n",
      "0.8935399532318116,0.047329614279388116 with: {'batch_size': 40, 'epochs': 50}\n",
      "0.90711350440979,0.05296420493420298 with: {'batch_size': 40, 'epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "# Summarize the results\n",
    "print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print('{},{} with: {}'.format(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[CV 1/5; 1/9] START dropout_rate=0.5, learning_rate=0.001.......................\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x000002188455E5E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 1/9] END .....dropout_rate=0.5, learning_rate=0.001; total time=   1.1s\n",
      "[CV 2/5; 1/9] START dropout_rate=0.5, learning_rate=0.001.......................\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882E5F0D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 2/5; 1/9] END .....dropout_rate=0.5, learning_rate=0.001; total time=   2.1s\n",
      "[CV 3/5; 1/9] START dropout_rate=0.5, learning_rate=0.001.......................\n",
      "WARNING:tensorflow:6 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x000002188455E790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 3/5; 1/9] END .....dropout_rate=0.5, learning_rate=0.001; total time=   0.9s\n",
      "[CV 4/5; 1/9] START dropout_rate=0.5, learning_rate=0.001.......................\n",
      "WARNING:tensorflow:7 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021885CC0430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 4/5; 1/9] END .....dropout_rate=0.5, learning_rate=0.001; total time=   1.4s\n",
      "[CV 5/5; 1/9] START dropout_rate=0.5, learning_rate=0.001.......................\n",
      "WARNING:tensorflow:7 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021886F033A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 1/9] END .....dropout_rate=0.5, learning_rate=0.001; total time=   1.9s\n",
      "[CV 1/5; 2/9] START dropout_rate=0.5, learning_rate=0.01........................\n",
      "WARNING:tensorflow:8 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x000002188455E1F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 2/9] END ......dropout_rate=0.5, learning_rate=0.01; total time=   2.2s\n",
      "[CV 2/5; 2/9] START dropout_rate=0.5, learning_rate=0.01........................\n",
      "WARNING:tensorflow:9 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882E5F8B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 2/5; 2/9] END ......dropout_rate=0.5, learning_rate=0.01; total time=   2.3s\n",
      "[CV 3/5; 2/9] START dropout_rate=0.5, learning_rate=0.01........................\n",
      "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218873B31F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 3/5; 2/9] END ......dropout_rate=0.5, learning_rate=0.01; total time=   2.5s\n",
      "[CV 4/5; 2/9] START dropout_rate=0.5, learning_rate=0.01........................\n",
      "WARNING:tensorflow:10 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882E5F4C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 4/5; 2/9] END ......dropout_rate=0.5, learning_rate=0.01; total time=   1.7s\n",
      "[CV 5/5; 2/9] START dropout_rate=0.5, learning_rate=0.01........................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021883116310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 2/9] END ......dropout_rate=0.5, learning_rate=0.01; total time=   1.4s\n",
      "[CV 1/5; 3/9] START dropout_rate=0.5, learning_rate=0.1.........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218873B3160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 3/9] END .......dropout_rate=0.5, learning_rate=0.1; total time=   1.6s\n",
      "[CV 2/5; 3/9] START dropout_rate=0.5, learning_rate=0.1.........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002188455E670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 2/5; 3/9] END .......dropout_rate=0.5, learning_rate=0.1; total time=   1.1s\n",
      "[CV 3/5; 3/9] START dropout_rate=0.5, learning_rate=0.1.........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218831CDE50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 3/5; 3/9] END .......dropout_rate=0.5, learning_rate=0.1; total time=   1.5s\n",
      "[CV 4/5; 3/9] START dropout_rate=0.5, learning_rate=0.1.........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218873B3310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 4/5; 3/9] END .......dropout_rate=0.5, learning_rate=0.1; total time=   1.3s\n",
      "[CV 5/5; 3/9] START dropout_rate=0.5, learning_rate=0.1.........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882BF73A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 3/9] END .......dropout_rate=0.5, learning_rate=0.1; total time=   1.4s\n",
      "[CV 1/5; 4/9] START dropout_rate=0.4, learning_rate=0.001.......................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021885A3B0D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 4/9] END .....dropout_rate=0.4, learning_rate=0.001; total time=   1.5s\n",
      "[CV 2/5; 4/9] START dropout_rate=0.4, learning_rate=0.001.......................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218831CDEE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 2/5; 4/9] END .....dropout_rate=0.4, learning_rate=0.001; total time=   0.9s\n",
      "[CV 3/5; 4/9] START dropout_rate=0.4, learning_rate=0.001.......................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882D89820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 3/5; 4/9] END .....dropout_rate=0.4, learning_rate=0.001; total time=   1.8s\n",
      "[CV 4/5; 4/9] START dropout_rate=0.4, learning_rate=0.001.......................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218831CDE50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 4/9] END .....dropout_rate=0.4, learning_rate=0.001; total time=   1.1s\n",
      "[CV 5/5; 4/9] START dropout_rate=0.4, learning_rate=0.001.......................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882E5F820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 4/9] END .....dropout_rate=0.4, learning_rate=0.001; total time=   1.2s\n",
      "[CV 1/5; 5/9] START dropout_rate=0.4, learning_rate=0.01........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002188746F040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 5/9] END ......dropout_rate=0.4, learning_rate=0.01; total time=   1.7s\n",
      "[CV 2/5; 5/9] START dropout_rate=0.4, learning_rate=0.01........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882FC15E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 2/5; 5/9] END ......dropout_rate=0.4, learning_rate=0.01; total time=   1.2s\n",
      "[CV 3/5; 5/9] START dropout_rate=0.4, learning_rate=0.01........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021887154E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 3/5; 5/9] END ......dropout_rate=0.4, learning_rate=0.01; total time=   1.5s\n",
      "[CV 4/5; 5/9] START dropout_rate=0.4, learning_rate=0.01........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021883116160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 4/5; 5/9] END ......dropout_rate=0.4, learning_rate=0.01; total time=   1.0s\n",
      "[CV 5/5; 5/9] START dropout_rate=0.4, learning_rate=0.01........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882FEE700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 5/9] END ......dropout_rate=0.4, learning_rate=0.01; total time=   1.6s\n",
      "[CV 1/5; 6/9] START dropout_rate=0.4, learning_rate=0.1.........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021883116F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 6/9] END .......dropout_rate=0.4, learning_rate=0.1; total time=   1.3s\n",
      "[CV 2/5; 6/9] START dropout_rate=0.4, learning_rate=0.1.........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218857C1160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 2/5; 6/9] END .......dropout_rate=0.4, learning_rate=0.1; total time=   1.1s\n",
      "[CV 3/5; 6/9] START dropout_rate=0.4, learning_rate=0.1.........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021887154F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 3/5; 6/9] END .......dropout_rate=0.4, learning_rate=0.1; total time=   1.7s\n",
      "[CV 4/5; 6/9] START dropout_rate=0.4, learning_rate=0.1.........................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882E5F160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 4/5; 6/9] END .......dropout_rate=0.4, learning_rate=0.1; total time=   0.9s\n",
      "[CV 5/5; 6/9] START dropout_rate=0.4, learning_rate=0.1.........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882FEE700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 6/9] END .......dropout_rate=0.4, learning_rate=0.1; total time=   1.4s\n",
      "[CV 1/5; 7/9] START dropout_rate=0.7, learning_rate=0.001.......................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002188464C310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 7/9] END .....dropout_rate=0.7, learning_rate=0.001; total time=   1.5s\n",
      "[CV 2/5; 7/9] START dropout_rate=0.7, learning_rate=0.001.......................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882E5FA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 2/5; 7/9] END .....dropout_rate=0.7, learning_rate=0.001; total time=   1.5s\n",
      "[CV 3/5; 7/9] START dropout_rate=0.7, learning_rate=0.001.......................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218831CD700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 3/5; 7/9] END .....dropout_rate=0.7, learning_rate=0.001; total time=   1.5s\n",
      "[CV 4/5; 7/9] START dropout_rate=0.7, learning_rate=0.001.......................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021884657AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 4/5; 7/9] END .....dropout_rate=0.7, learning_rate=0.001; total time=   1.1s\n",
      "[CV 5/5; 7/9] START dropout_rate=0.7, learning_rate=0.001.......................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218831CDA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 7/9] END .....dropout_rate=0.7, learning_rate=0.001; total time=   1.5s\n",
      "[CV 1/5; 8/9] START dropout_rate=0.7, learning_rate=0.01........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002188455E3A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 8/9] END ......dropout_rate=0.7, learning_rate=0.01; total time=   1.1s\n",
      "[CV 2/5; 8/9] START dropout_rate=0.7, learning_rate=0.01........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218871481F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 2/5; 8/9] END ......dropout_rate=0.7, learning_rate=0.01; total time=   1.2s\n",
      "[CV 3/5; 8/9] START dropout_rate=0.7, learning_rate=0.01........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218831B6940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 8/9] END ......dropout_rate=0.7, learning_rate=0.01; total time=   1.7s\n",
      "[CV 4/5; 8/9] START dropout_rate=0.7, learning_rate=0.01........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218857C1160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 4/5; 8/9] END ......dropout_rate=0.7, learning_rate=0.01; total time=   1.0s\n",
      "[CV 5/5; 8/9] START dropout_rate=0.7, learning_rate=0.01........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882FEE9D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 8/9] END ......dropout_rate=0.7, learning_rate=0.01; total time=   1.5s\n",
      "[CV 1/5; 9/9] START dropout_rate=0.7, learning_rate=0.1.........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021887154DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 9/9] END .......dropout_rate=0.7, learning_rate=0.1; total time=   1.2s\n",
      "[CV 2/5; 9/9] START dropout_rate=0.7, learning_rate=0.1.........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882FEE790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 2/5; 9/9] END .......dropout_rate=0.7, learning_rate=0.1; total time=   1.4s\n",
      "[CV 3/5; 9/9] START dropout_rate=0.7, learning_rate=0.1.........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021885A3B160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 3/5; 9/9] END .......dropout_rate=0.7, learning_rate=0.1; total time=   1.8s\n",
      "[CV 4/5; 9/9] START dropout_rate=0.7, learning_rate=0.1.........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882E5FA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 4/5; 9/9] END .......dropout_rate=0.7, learning_rate=0.1; total time=   1.6s\n",
      "[CV 5/5; 9/9] START dropout_rate=0.7, learning_rate=0.1.........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882FEEC10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 9/9] END .......dropout_rate=0.7, learning_rate=0.1; total time=   2.5s\n"
     ]
    }
   ],
   "source": [
    "#### Tuning of Hyperparameters:- Learning rate and Drop out rate\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# Defining the model\n",
    "\n",
    "def create_model(learning_rate,dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(28,input_dim = 28,kernel_initializer = 'normal',activation = 'relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(20,input_dim = 28,kernel_initializer = 'normal',activation = 'relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1,activation = 'sigmoid'))\n",
    "    \n",
    "    adam = Adam(lr = learning_rate)\n",
    "    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 200,epochs = 10)\n",
    "\n",
    "# Define the grid search parameters\n",
    "\n",
    "learning_rate = [0.001,0.01,0.1]\n",
    "dropout_rate = [0.5,0.4,0.7]\n",
    "\n",
    "# Make a dictionary of the grid search parameters\n",
    "\n",
    "param_grids = dict(learning_rate = learning_rate,dropout_rate = dropout_rate)\n",
    "\n",
    "# Build and fit the GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(estimator = model,param_grid = param_grids,cv = KFold(),verbose = 10)\n",
    "grid_result = grid.fit(X_standardized,Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best : 0.734466016292572, using {'dropout_rate': 0.5, 'learning_rate': 0.01}\n",
      "0.7305825233459473,0.15435061319000673 with: {'dropout_rate': 0.5, 'learning_rate': 0.001}\n",
      "0.734466016292572,0.1537509780062806 with: {'dropout_rate': 0.5, 'learning_rate': 0.01}\n",
      "0.7305825233459473,0.15435061319000673 with: {'dropout_rate': 0.5, 'learning_rate': 0.1}\n",
      "0.7305825233459473,0.15435061319000673 with: {'dropout_rate': 0.4, 'learning_rate': 0.001}\n",
      "0.7210044741630555,0.13120679055184203 with: {'dropout_rate': 0.4, 'learning_rate': 0.01}\n",
      "0.7306198596954345,0.14247324961509483 with: {'dropout_rate': 0.4, 'learning_rate': 0.1}\n",
      "0.7305825233459473,0.15435061319000673 with: {'dropout_rate': 0.7, 'learning_rate': 0.001}\n",
      "0.7305825233459473,0.15435061319000673 with: {'dropout_rate': 0.7, 'learning_rate': 0.01}\n",
      "0.7305825233459473,0.15435061319000673 with: {'dropout_rate': 0.7, 'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Summarize the results\n",
    "print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print('{},{} with: {}'.format(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV 1/5; 1/12] START activation_function=softmax, init=uniform..................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021885BB4820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 1/12] END activation_function=softmax, init=uniform; total time=   3.6s\n",
      "[CV 2/5; 1/12] START activation_function=softmax, init=uniform..................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218831CD160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 2/5; 1/12] END activation_function=softmax, init=uniform; total time=   1.2s\n",
      "[CV 3/5; 1/12] START activation_function=softmax, init=uniform..................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002188464C700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 3/5; 1/12] END activation_function=softmax, init=uniform; total time=   1.9s\n",
      "[CV 4/5; 1/12] START activation_function=softmax, init=uniform..................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218887A7CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 4/5; 1/12] END activation_function=softmax, init=uniform; total time=   1.2s\n",
      "[CV 5/5; 1/12] START activation_function=softmax, init=uniform..................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218857C10D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 1/12] END activation_function=softmax, init=uniform; total time=   2.1s\n",
      "[CV 1/5; 2/12] START activation_function=softmax, init=normal...................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021887154670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 2/12] END .activation_function=softmax, init=normal; total time=   1.5s\n",
      "[CV 2/5; 2/12] START activation_function=softmax, init=normal...................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218831CD8B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 2/5; 2/12] END .activation_function=softmax, init=normal; total time=   1.1s\n",
      "[CV 3/5; 2/12] START activation_function=softmax, init=normal...................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882FEE820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 3/5; 2/12] END .activation_function=softmax, init=normal; total time=   2.0s\n",
      "[CV 4/5; 2/12] START activation_function=softmax, init=normal...................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882E5F5E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 4/5; 2/12] END .activation_function=softmax, init=normal; total time=   0.9s\n",
      "[CV 5/5; 2/12] START activation_function=softmax, init=normal...................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021885BB4160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 2/12] END .activation_function=softmax, init=normal; total time=   1.6s\n",
      "[CV 1/5; 3/12] START activation_function=softmax, init=zero.....................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002188455E700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 3/12] END ...activation_function=softmax, init=zero; total time=   1.4s\n",
      "[CV 2/5; 3/12] START activation_function=softmax, init=zero.....................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218857C1700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 2/5; 3/12] END ...activation_function=softmax, init=zero; total time=   1.4s\n",
      "[CV 3/5; 3/12] START activation_function=softmax, init=zero.....................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021887154160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 3/5; 3/12] END ...activation_function=softmax, init=zero; total time=   1.7s\n",
      "[CV 4/5; 3/12] START activation_function=softmax, init=zero.....................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882E5F9D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 4/5; 3/12] END ...activation_function=softmax, init=zero; total time=   1.2s\n",
      "[CV 5/5; 3/12] START activation_function=softmax, init=zero.....................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218831B69D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 3/12] END ...activation_function=softmax, init=zero; total time=   1.9s\n",
      "[CV 1/5; 4/12] START activation_function=relu, init=uniform.....................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021885A3BA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 4/12] END ...activation_function=relu, init=uniform; total time=   1.0s\n",
      "[CV 2/5; 4/12] START activation_function=relu, init=uniform.....................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021886EED280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 2/5; 4/12] END ...activation_function=relu, init=uniform; total time=   1.6s\n",
      "[CV 3/5; 4/12] START activation_function=relu, init=uniform.....................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021885A3BCA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 3/5; 4/12] END ...activation_function=relu, init=uniform; total time=   1.3s\n",
      "[CV 4/5; 4/12] START activation_function=relu, init=uniform.....................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218871549D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 4/12] END ...activation_function=relu, init=uniform; total time=   1.0s\n",
      "[CV 5/5; 4/12] START activation_function=relu, init=uniform.....................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882FEE940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 4/12] END ...activation_function=relu, init=uniform; total time=   1.7s\n",
      "[CV 1/5; 5/12] START activation_function=relu, init=normal......................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002188464CE50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 5/12] END ....activation_function=relu, init=normal; total time=   1.2s\n",
      "[CV 2/5; 5/12] START activation_function=relu, init=normal......................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021887154670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 2/5; 5/12] END ....activation_function=relu, init=normal; total time=   1.4s\n",
      "[CV 3/5; 5/12] START activation_function=relu, init=normal......................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021885A3BCA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 3/5; 5/12] END ....activation_function=relu, init=normal; total time=   1.3s\n",
      "[CV 4/5; 5/12] START activation_function=relu, init=normal......................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882FEE940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 4/5; 5/12] END ....activation_function=relu, init=normal; total time=   1.3s\n",
      "[CV 5/5; 5/12] START activation_function=relu, init=normal......................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882E5F040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 5/12] END ....activation_function=relu, init=normal; total time=   1.7s\n",
      "[CV 1/5; 6/12] START activation_function=relu, init=zero........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218831CD160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 6/12] END ......activation_function=relu, init=zero; total time=   1.1s\n",
      "[CV 2/5; 6/12] START activation_function=relu, init=zero........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002188464CF70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 2/5; 6/12] END ......activation_function=relu, init=zero; total time=   1.7s\n",
      "[CV 3/5; 6/12] START activation_function=relu, init=zero........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021883116310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 3/5; 6/12] END ......activation_function=relu, init=zero; total time=   1.2s\n",
      "[CV 4/5; 6/12] START activation_function=relu, init=zero........................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882FEEAF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 4/5; 6/12] END ......activation_function=relu, init=zero; total time=   1.1s\n",
      "[CV 5/5; 6/12] START activation_function=relu, init=zero........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218831B6700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 6/12] END ......activation_function=relu, init=zero; total time=   1.8s\n",
      "[CV 1/5; 7/12] START activation_function=tanh, init=uniform.....................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021885A3B550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 7/12] END ...activation_function=tanh, init=uniform; total time=   0.9s\n",
      "[CV 2/5; 7/12] START activation_function=tanh, init=uniform.....................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218831164C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 2/5; 7/12] END ...activation_function=tanh, init=uniform; total time=   1.6s\n",
      "[CV 3/5; 7/12] START activation_function=tanh, init=uniform.....................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021885A9BD30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 3/5; 7/12] END ...activation_function=tanh, init=uniform; total time=   1.4s\n",
      "[CV 4/5; 7/12] START activation_function=tanh, init=uniform.....................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218831B6700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 4/5; 7/12] END ...activation_function=tanh, init=uniform; total time=   1.4s\n",
      "[CV 5/5; 7/12] START activation_function=tanh, init=uniform.....................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882FEEAF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 7/12] END ...activation_function=tanh, init=uniform; total time=   2.0s\n",
      "[CV 1/5; 8/12] START activation_function=tanh, init=normal......................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218875D2670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 8/12] END ....activation_function=tanh, init=normal; total time=   1.8s\n",
      "[CV 2/5; 8/12] START activation_function=tanh, init=normal......................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882E5F820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 2/5; 8/12] END ....activation_function=tanh, init=normal; total time=   3.4s\n",
      "[CV 3/5; 8/12] START activation_function=tanh, init=normal......................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021885A3B790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 8/12] END ....activation_function=tanh, init=normal; total time=   1.6s\n",
      "[CV 4/5; 8/12] START activation_function=tanh, init=normal......................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218857C1C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 4/5; 8/12] END ....activation_function=tanh, init=normal; total time=   1.6s\n",
      "[CV 5/5; 8/12] START activation_function=tanh, init=normal......................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021883116280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 8/12] END ....activation_function=tanh, init=normal; total time=   1.2s\n",
      "[CV 1/5; 9/12] START activation_function=tanh, init=zero........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218831B6940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 9/12] END ......activation_function=tanh, init=zero; total time=   1.6s\n",
      "[CV 2/5; 9/12] START activation_function=tanh, init=zero........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218871549D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 2/5; 9/12] END ......activation_function=tanh, init=zero; total time=   1.3s\n",
      "[CV 3/5; 9/12] START activation_function=tanh, init=zero........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021886E47D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 3/5; 9/12] END ......activation_function=tanh, init=zero; total time=   1.4s\n",
      "[CV 4/5; 9/12] START activation_function=tanh, init=zero........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002188464CD30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 4/5; 9/12] END ......activation_function=tanh, init=zero; total time=   1.7s\n",
      "[CV 5/5; 9/12] START activation_function=tanh, init=zero........................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218857C1700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 9/12] END ......activation_function=tanh, init=zero; total time=   1.1s\n",
      "[CV 1/5; 10/12] START activation_function=linear, init=uniform..................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218831164C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 10/12] END activation_function=linear, init=uniform; total time=   1.5s\n",
      "[CV 2/5; 10/12] START activation_function=linear, init=uniform..................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882FEE790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 2/5; 10/12] END activation_function=linear, init=uniform; total time=   1.3s\n",
      "[CV 3/5; 10/12] START activation_function=linear, init=uniform..................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218857C1B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 3/5; 10/12] END activation_function=linear, init=uniform; total time=   1.2s\n",
      "[CV 4/5; 10/12] START activation_function=linear, init=uniform..................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021887154040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 4/5; 10/12] END activation_function=linear, init=uniform; total time=   1.4s\n",
      "[CV 5/5; 10/12] START activation_function=linear, init=uniform..................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002188464C9D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 10/12] END activation_function=linear, init=uniform; total time=   1.1s\n",
      "[CV 1/5; 11/12] START activation_function=linear, init=normal...................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021885A3B940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 11/12] END .activation_function=linear, init=normal; total time=   1.5s\n",
      "[CV 2/5; 11/12] START activation_function=linear, init=normal...................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002188455E3A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 2/5; 11/12] END .activation_function=linear, init=normal; total time=   1.1s\n",
      "[CV 3/5; 11/12] START activation_function=linear, init=normal...................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002188455E790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 3/5; 11/12] END .activation_function=linear, init=normal; total time=   1.3s\n",
      "[CV 4/5; 11/12] START activation_function=linear, init=normal...................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021883116310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 4/5; 11/12] END .activation_function=linear, init=normal; total time=   1.6s\n",
      "[CV 5/5; 11/12] START activation_function=linear, init=normal...................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002188464CF70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 11/12] END .activation_function=linear, init=normal; total time=   0.8s\n",
      "[CV 1/5; 12/12] START activation_function=linear, init=zero.....................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021885B2D160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 12/12] END ...activation_function=linear, init=zero; total time=   1.8s\n",
      "[CV 2/5; 12/12] START activation_function=linear, init=zero.....................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882E5F790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 12/12] END ...activation_function=linear, init=zero; total time=   1.4s\n",
      "[CV 3/5; 12/12] START activation_function=linear, init=zero.....................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218857C1430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 3/5; 12/12] END ...activation_function=linear, init=zero; total time=   1.2s\n",
      "[CV 4/5; 12/12] START activation_function=linear, init=zero.....................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021885A3B1F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 4/5; 12/12] END ...activation_function=linear, init=zero; total time=   1.4s\n",
      "[CV 5/5; 12/12] START activation_function=linear, init=zero.....................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002188455E160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 12/12] END ...activation_function=linear, init=zero; total time=   1.1s\n"
     ]
    }
   ],
   "source": [
    "#### Tuning of Hyperparameters:- Activation Function and Kernel Initializer\n",
    "# Defining the model\n",
    "\n",
    "def create_model(activation_function,init):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(28,input_dim = 28,kernel_initializer = init,activation = activation_function))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(20,input_dim = 28,kernel_initializer = init,activation = activation_function))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1,activation = 'sigmoid'))\n",
    "    \n",
    "    adam = Adam(lr = 0.001)\n",
    "    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 200,epochs = 10)\n",
    "\n",
    "# Define the grid search parameters\n",
    "activation_function = ['softmax','relu','tanh','linear']\n",
    "init = ['uniform','normal','zero']\n",
    "\n",
    "# Make a dictionary of the grid search parameters\n",
    "param_grids = dict(activation_function = activation_function,init = init)\n",
    "\n",
    "# Build and fit the GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(estimator = model,param_grid = param_grids,cv = KFold(),verbose = 10)\n",
    "grid_result = grid.fit(X_standardized,Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best : 0.7326549768447876, using {'activation_function': 'linear', 'init': 'uniform'}\n",
      "0.5694174766540527,0.2686514946324045 with: {'activation_function': 'softmax', 'init': 'uniform'}\n",
      "0.7305825233459473,0.15435061319000673 with: {'activation_function': 'softmax', 'init': 'normal'}\n",
      "0.7305825233459473,0.15435061319000673 with: {'activation_function': 'softmax', 'init': 'zero'}\n",
      "0.7305825233459473,0.15435061319000673 with: {'activation_function': 'relu', 'init': 'uniform'}\n",
      "0.7305825233459473,0.15435061319000673 with: {'activation_function': 'relu', 'init': 'normal'}\n",
      "0.7305825233459473,0.15435061319000673 with: {'activation_function': 'relu', 'init': 'zero'}\n",
      "0.7132001399993897,0.13122338718611995 with: {'activation_function': 'tanh', 'init': 'uniform'}\n",
      "0.7094846963882446,0.10886230557509569 with: {'activation_function': 'tanh', 'init': 'normal'}\n",
      "0.7305825233459473,0.15435061319000673 with: {'activation_function': 'tanh', 'init': 'zero'}\n",
      "0.7326549768447876,0.11795652731312903 with: {'activation_function': 'linear', 'init': 'uniform'}\n",
      "0.688181471824646,0.10512772975788758 with: {'activation_function': 'linear', 'init': 'normal'}\n",
      "0.7305825233459473,0.15435061319000673 with: {'activation_function': 'linear', 'init': 'zero'}\n"
     ]
    }
   ],
   "source": [
    "# Summarize the results\n",
    "print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print('{},{} with: {}'.format(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[CV 1/5; 1/9] START neuron1=4, neuron2=2........................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882057430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 1/9] END ......................neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 2/5; 1/9] START neuron1=4, neuron2=2........................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218F8ABBAF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 2/5; 1/9] END ......................neuron1=4, neuron2=2; total time=   1.4s\n",
      "[CV 3/5; 1/9] START neuron1=4, neuron2=2........................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882D89550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 3/5; 1/9] END ......................neuron1=4, neuron2=2; total time=   1.5s\n",
      "[CV 4/5; 1/9] START neuron1=4, neuron2=2........................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218887168B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 4/5; 1/9] END ......................neuron1=4, neuron2=2; total time=   0.9s\n",
      "[CV 5/5; 1/9] START neuron1=4, neuron2=2........................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882FEE700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 1/9] END ......................neuron1=4, neuron2=2; total time=   1.9s\n",
      "[CV 1/5; 2/9] START neuron1=4, neuron2=4........................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218F8ABBB80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 2/9] END ......................neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 2/5; 2/9] START neuron1=4, neuron2=4........................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002188455E160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 2/5; 2/9] END ......................neuron1=4, neuron2=4; total time=   1.2s\n",
      "[CV 3/5; 2/9] START neuron1=4, neuron2=4........................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218857C1F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 3/5; 2/9] END ......................neuron1=4, neuron2=4; total time=   1.5s\n",
      "[CV 4/5; 2/9] START neuron1=4, neuron2=4........................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002188455E430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 4/5; 2/9] END ......................neuron1=4, neuron2=4; total time=   1.2s\n",
      "[CV 5/5; 2/9] START neuron1=4, neuron2=4........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882FC1820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 2/9] END ......................neuron1=4, neuron2=4; total time=   1.6s\n",
      "[CV 1/5; 3/9] START neuron1=4, neuron2=8........................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021888716280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 3/9] END ......................neuron1=4, neuron2=8; total time=   1.1s\n",
      "[CV 2/5; 3/9] START neuron1=4, neuron2=8........................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882D89670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 2/5; 3/9] END ......................neuron1=4, neuron2=8; total time=   2.0s\n",
      "[CV 3/5; 3/9] START neuron1=4, neuron2=8........................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882057940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 3/5; 3/9] END ......................neuron1=4, neuron2=8; total time=   2.1s\n",
      "[CV 4/5; 3/9] START neuron1=4, neuron2=8........................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882E5F820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 4/5; 3/9] END ......................neuron1=4, neuron2=8; total time=   2.5s\n",
      "[CV 5/5; 3/9] START neuron1=4, neuron2=8........................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002188464C9D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 3/9] END ......................neuron1=4, neuron2=8; total time=   3.2s\n",
      "[CV 1/5; 4/9] START neuron1=8, neuron2=2........................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021881FF9CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 4/9] END ......................neuron1=8, neuron2=2; total time=   1.5s\n",
      "[CV 2/5; 4/9] START neuron1=8, neuron2=2........................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021888716550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 2/5; 4/9] END ......................neuron1=8, neuron2=2; total time=   1.7s\n",
      "[CV 3/5; 4/9] START neuron1=8, neuron2=2........................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218857C1E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 3/5; 4/9] END ......................neuron1=8, neuron2=2; total time=   1.1s\n",
      "[CV 4/5; 4/9] START neuron1=8, neuron2=2........................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882D89C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 4/9] END ......................neuron1=8, neuron2=2; total time=   1.9s\n",
      "[CV 5/5; 4/9] START neuron1=8, neuron2=2........................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882FEE8B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 4/9] END ......................neuron1=8, neuron2=2; total time=   1.2s\n",
      "[CV 1/5; 5/9] START neuron1=8, neuron2=4........................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882E5F430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 5/9] END ......................neuron1=8, neuron2=4; total time=   1.8s\n",
      "[CV 2/5; 5/9] START neuron1=8, neuron2=4........................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882FEEE50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 2/5; 5/9] END ......................neuron1=8, neuron2=4; total time=   1.7s\n",
      "[CV 3/5; 5/9] START neuron1=8, neuron2=4........................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882E5FA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 3/5; 5/9] END ......................neuron1=8, neuron2=4; total time=   1.2s\n",
      "[CV 4/5; 5/9] START neuron1=8, neuron2=4........................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882D894C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 4/5; 5/9] END ......................neuron1=8, neuron2=4; total time=   1.9s\n",
      "[CV 5/5; 5/9] START neuron1=8, neuron2=4........................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218F8ABBB80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 5/9] END ......................neuron1=8, neuron2=4; total time=   1.1s\n",
      "[CV 1/5; 6/9] START neuron1=8, neuron2=8........................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218820579D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 6/9] END ......................neuron1=8, neuron2=8; total time=   1.6s\n",
      "[CV 2/5; 6/9] START neuron1=8, neuron2=8........................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218831CD790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 2/5; 6/9] END ......................neuron1=8, neuron2=8; total time=   1.3s\n",
      "[CV 3/5; 6/9] START neuron1=8, neuron2=8........................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882E5F790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 3/5; 6/9] END ......................neuron1=8, neuron2=8; total time=   1.4s\n",
      "[CV 4/5; 6/9] START neuron1=8, neuron2=8........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021887154820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 4/5; 6/9] END ......................neuron1=8, neuron2=8; total time=   1.4s\n",
      "[CV 5/5; 6/9] START neuron1=8, neuron2=8........................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882D89820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 6/9] END ......................neuron1=8, neuron2=8; total time=   0.9s\n",
      "[CV 1/5; 7/9] START neuron1=16, neuron2=2.......................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218843F9790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 7/9] END .....................neuron1=16, neuron2=2; total time=   1.8s\n",
      "[CV 2/5; 7/9] START neuron1=16, neuron2=2.......................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218831CDD30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 2/5; 7/9] END .....................neuron1=16, neuron2=2; total time=   1.1s\n",
      "[CV 3/5; 7/9] START neuron1=16, neuron2=2.......................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021888716280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 3/5; 7/9] END .....................neuron1=16, neuron2=2; total time=   1.2s\n",
      "[CV 4/5; 7/9] START neuron1=16, neuron2=2.......................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882E5F3A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 4/5; 7/9] END .....................neuron1=16, neuron2=2; total time=   1.6s\n",
      "[CV 5/5; 7/9] START neuron1=16, neuron2=2.......................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882FC18B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 7/9] END .....................neuron1=16, neuron2=2; total time=   1.4s\n",
      "[CV 1/5; 8/9] START neuron1=16, neuron2=4.......................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882057040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 8/9] END .....................neuron1=16, neuron2=4; total time=   1.8s\n",
      "[CV 2/5; 8/9] START neuron1=16, neuron2=4.......................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218831CD9D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 2/5; 8/9] END .....................neuron1=16, neuron2=4; total time=   1.2s\n",
      "[CV 3/5; 8/9] START neuron1=16, neuron2=4.......................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882FC1A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 8/9] END .....................neuron1=16, neuron2=4; total time=   1.9s\n",
      "[CV 4/5; 8/9] START neuron1=16, neuron2=4.......................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021888716CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 4/5; 8/9] END .....................neuron1=16, neuron2=4; total time=   1.5s\n",
      "[CV 5/5; 8/9] START neuron1=16, neuron2=4.......................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002188464C550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 8/9] END .....................neuron1=16, neuron2=4; total time=   1.2s\n",
      "[CV 1/5; 9/9] START neuron1=16, neuron2=8.......................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218F8BB3C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 9/9] END .....................neuron1=16, neuron2=8; total time=   2.2s\n",
      "[CV 2/5; 9/9] START neuron1=16, neuron2=8.......................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882E5F160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 2/5; 9/9] END .....................neuron1=16, neuron2=8; total time=   1.2s\n",
      "[CV 3/5; 9/9] START neuron1=16, neuron2=8.......................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x000002188455E0D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 3/5; 9/9] END .....................neuron1=16, neuron2=8; total time=   1.8s\n",
      "[CV 4/5; 9/9] START neuron1=16, neuron2=8.......................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882E8C1F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 4/5; 9/9] END .....................neuron1=16, neuron2=8; total time=   1.8s\n",
      "[CV 5/5; 9/9] START neuron1=16, neuron2=8.......................................\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x0000021882057430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 5/5; 9/9] END .....................neuron1=16, neuron2=8; total time=   1.5s\n"
     ]
    }
   ],
   "source": [
    "#### Tuning of Hyperparameter :-Number of Neurons in activation layer\n",
    "# Defining the model\n",
    "\n",
    "def create_model(neuron1,neuron2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neuron1,input_dim = 28,kernel_initializer = 'uniform',activation = 'tanh'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(neuron2,input_dim = neuron1,kernel_initializer = 'uniform',activation = 'tanh'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1,activation = 'sigmoid'))\n",
    "    \n",
    "    adam = Adam(lr = 0.001)\n",
    "    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 200,epochs = 10)\n",
    "\n",
    "# Define the grid search parameters\n",
    "\n",
    "neuron1 = [4,8,16]\n",
    "neuron2 = [2,4,8]\n",
    "\n",
    "# Make a dictionary of the grid search parameters\n",
    "\n",
    "param_grids = dict(neuron1 = neuron1,neuron2 = neuron2)\n",
    "\n",
    "# Build and fit the GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(estimator = model,param_grid = param_grids,cv = KFold(),verbose = 10)\n",
    "grid_result = grid.fit(X_standardized,Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best : 0.734466016292572, using {'neuron1': 4, 'neuron2': 4}\n",
      "0.7325242638587952,0.15400213076804797 with: {'neuron1': 4, 'neuron2': 2}\n",
      "0.734466016292572,0.1537509780062806 with: {'neuron1': 4, 'neuron2': 4}\n",
      "0.7305825233459473,0.15435061319000673 with: {'neuron1': 4, 'neuron2': 8}\n",
      "0.7209297895431519,0.1537378788889947 with: {'neuron1': 8, 'neuron2': 2}\n",
      "0.7131814658641815,0.15270931012094896 with: {'neuron1': 8, 'neuron2': 4}\n",
      "0.7305825233459473,0.15916112736681773 with: {'neuron1': 8, 'neuron2': 8}\n",
      "0.6960604786872864,0.08280163608155632 with: {'neuron1': 16, 'neuron2': 2}\n",
      "0.7287714719772339,0.1155195932853835 with: {'neuron1': 16, 'neuron2': 4}\n",
      "0.7286967873573303,0.14213084407037793 with: {'neuron1': 16, 'neuron2': 8}\n"
     ]
    }
   ],
   "source": [
    "# Summarize the results\n",
    "print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print('{},{} with: {}'.format(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Darshan\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7620889748549323\n"
     ]
    }
   ],
   "source": [
    "#### Training model with optimum values of Hyperparameters\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Defining the model\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16,input_dim = 28,kernel_initializer = 'uniform',activation = 'tanh'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(4,input_dim = 16,kernel_initializer = 'uniform',activation = 'tanh'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1,activation = 'sigmoid'))\n",
    "    \n",
    "    adam = Adam(lr = 0.001) #sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 200,epochs = 10)\n",
    "\n",
    "# Fitting the model\n",
    "\n",
    "model.fit(X_standardized,Y)\n",
    "\n",
    "# Predicting using trained model\n",
    "\n",
    "y_predict = model.predict(X_standardized)\n",
    "\n",
    "# Printing the metrics\n",
    "print(accuracy_score(Y,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8748 candidates, totalling 43740 fits\n",
      "[CV 1/5; 1/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000218831B6B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[CV 1/5; 1/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2; total time=   3.3s\n",
      "[CV 2/5; 1/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 2/5; 1/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2; total time=   2.8s\n",
      "[CV 3/5; 1/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 3/5; 1/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2; total time=   3.1s\n",
      "[CV 4/5; 1/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 4/5; 1/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2; total time=   1.8s\n",
      "[CV 5/5; 1/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 5/5; 1/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2; total time=   1.8s\n",
      "[CV 1/5; 2/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 1/5; 2/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 2/5; 2/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 2/5; 2/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4; total time=   2.1s\n",
      "[CV 3/5; 2/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 3/5; 2/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4; total time=   1.2s\n",
      "[CV 4/5; 2/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 4/5; 2/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4; total time=   1.9s\n",
      "[CV 5/5; 2/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 5/5; 2/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4; total time=   1.8s\n",
      "[CV 1/5; 3/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 1/5; 3/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8; total time=   2.5s\n",
      "[CV 2/5; 3/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 2/5; 3/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8; total time=   1.9s\n",
      "[CV 3/5; 3/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 3/5; 3/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8; total time=   2.3s\n",
      "[CV 4/5; 3/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 4/5; 3/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8; total time=   1.5s\n",
      "[CV 5/5; 3/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 5/5; 3/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8; total time=   2.2s\n",
      "[CV 1/5; 4/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 1/5; 4/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2; total time=   1.8s\n",
      "[CV 2/5; 4/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 2/5; 4/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2; total time=   2.3s\n",
      "[CV 3/5; 4/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 3/5; 4/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2; total time=   1.4s\n",
      "[CV 4/5; 4/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 4/5; 4/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2; total time=   2.0s\n",
      "[CV 5/5; 4/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 5/5; 4/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2; total time=   2.0s\n",
      "[CV 1/5; 5/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n",
      "[CV 1/5; 5/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4; total time=   1.7s\n",
      "[CV 2/5; 5/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n",
      "[CV 2/5; 5/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4; total time=   1.6s\n",
      "[CV 3/5; 5/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n",
      "[CV 3/5; 5/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4; total time=   1.9s\n",
      "[CV 4/5; 5/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 5/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4; total time=   1.8s\n",
      "[CV 5/5; 5/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n",
      "[CV 5/5; 5/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4; total time=   1.6s\n",
      "[CV 1/5; 6/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n",
      "[CV 1/5; 6/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8; total time=   1.9s\n",
      "[CV 2/5; 6/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n",
      "[CV 2/5; 6/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8; total time=   1.5s\n",
      "[CV 3/5; 6/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n",
      "[CV 3/5; 6/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8; total time=   2.4s\n",
      "[CV 4/5; 6/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n",
      "[CV 4/5; 6/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8; total time=   2.4s\n",
      "[CV 5/5; 6/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n",
      "[CV 5/5; 6/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8; total time=   3.0s\n",
      "[CV 1/5; 7/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2\n",
      "[CV 1/5; 7/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2; total time=   2.8s\n",
      "[CV 2/5; 7/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2\n",
      "[CV 2/5; 7/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2; total time=   1.9s\n",
      "[CV 3/5; 7/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2\n",
      "[CV 3/5; 7/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2; total time=   1.8s\n",
      "[CV 4/5; 7/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2\n",
      "[CV 4/5; 7/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2; total time=   1.5s\n",
      "[CV 5/5; 7/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2\n",
      "[CV 5/5; 7/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2; total time=   2.2s\n",
      "[CV 1/5; 8/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4\n",
      "[CV 1/5; 8/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4; total time=   1.5s\n",
      "[CV 2/5; 8/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4\n",
      "[CV 2/5; 8/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4; total time=   1.7s\n",
      "[CV 3/5; 8/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4\n",
      "[CV 3/5; 8/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4; total time=   1.6s\n",
      "[CV 4/5; 8/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4\n",
      "[CV 4/5; 8/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4; total time=   1.8s\n",
      "[CV 5/5; 8/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4\n",
      "[CV 5/5; 8/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4; total time=   1.7s\n",
      "[CV 1/5; 9/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8\n",
      "[CV 1/5; 9/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8; total time=   1.6s\n",
      "[CV 2/5; 9/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8\n",
      "[CV 2/5; 9/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8; total time=   2.1s\n",
      "[CV 3/5; 9/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8\n",
      "[CV 3/5; 9/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8; total time=   1.9s\n",
      "[CV 4/5; 9/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8\n",
      "[CV 4/5; 9/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8; total time=   2.0s\n",
      "[CV 5/5; 9/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8\n",
      "[CV 5/5; 9/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8; total time=   1.7s\n",
      "[CV 1/5; 10/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 10/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.7s\n",
      "[CV 2/5; 10/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 10/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 3/5; 10/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 10/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   2.0s\n",
      "[CV 4/5; 10/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 10/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.3s\n",
      "[CV 5/5; 10/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 10/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.8s\n",
      "[CV 1/5; 11/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 11/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.5s\n",
      "[CV 2/5; 11/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 11/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.9s\n",
      "[CV 3/5; 11/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 11/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.5s\n",
      "[CV 4/5; 11/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 11/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   2.1s\n",
      "[CV 5/5; 11/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 11/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.4s\n",
      "[CV 1/5; 12/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8\n",
      "[CV 1/5; 12/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8; total time=   1.8s\n",
      "[CV 2/5; 12/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8\n",
      "[CV 2/5; 12/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8; total time=   1.5s\n",
      "[CV 3/5; 12/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8\n",
      "[CV 3/5; 12/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8; total time=   2.2s\n",
      "[CV 4/5; 12/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8\n",
      "[CV 4/5; 12/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8; total time=   1.6s\n",
      "[CV 5/5; 12/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8\n",
      "[CV 5/5; 12/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8; total time=   2.5s\n",
      "[CV 1/5; 13/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 13/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   2.7s\n",
      "[CV 2/5; 13/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 13/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   3.5s\n",
      "[CV 3/5; 13/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 13/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.5s\n",
      "[CV 4/5; 13/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 13/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.9s\n",
      "[CV 5/5; 13/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 13/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 1/5; 14/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 14/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.7s\n",
      "[CV 2/5; 14/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 14/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.4s\n",
      "[CV 3/5; 14/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 14/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   2.2s\n",
      "[CV 4/5; 14/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 14/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.4s\n",
      "[CV 5/5; 14/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 14/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   2.0s\n",
      "[CV 1/5; 15/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8\n",
      "[CV 1/5; 15/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8; total time=   1.4s\n",
      "[CV 2/5; 15/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8\n",
      "[CV 2/5; 15/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8; total time=   1.8s\n",
      "[CV 3/5; 15/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8\n",
      "[CV 3/5; 15/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8; total time=   1.7s\n",
      "[CV 4/5; 15/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8\n",
      "[CV 4/5; 15/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8; total time=   1.7s\n",
      "[CV 5/5; 15/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8\n",
      "[CV 5/5; 15/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8; total time=   1.5s\n",
      "[CV 1/5; 16/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 16/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2; total time=   1.5s\n",
      "[CV 2/5; 16/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2\n",
      "[CV 2/5; 16/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2; total time=   2.4s\n",
      "[CV 3/5; 16/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2\n",
      "[CV 3/5; 16/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2; total time=   1.7s\n",
      "[CV 4/5; 16/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2\n",
      "[CV 4/5; 16/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2; total time=   1.8s\n",
      "[CV 5/5; 16/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2\n",
      "[CV 5/5; 16/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2; total time=   1.7s\n",
      "[CV 1/5; 17/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4\n",
      "[CV 1/5; 17/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4; total time=   1.7s\n",
      "[CV 2/5; 17/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4\n",
      "[CV 2/5; 17/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4; total time=   1.3s\n",
      "[CV 3/5; 17/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4\n",
      "[CV 3/5; 17/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4; total time=   2.1s\n",
      "[CV 4/5; 17/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4\n",
      "[CV 4/5; 17/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4; total time=   1.3s\n",
      "[CV 5/5; 17/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4\n",
      "[CV 5/5; 17/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4; total time=   1.9s\n",
      "[CV 1/5; 18/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8\n",
      "[CV 1/5; 18/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8; total time=   1.8s\n",
      "[CV 2/5; 18/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8\n",
      "[CV 2/5; 18/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8; total time=   1.8s\n",
      "[CV 3/5; 18/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8\n",
      "[CV 3/5; 18/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8; total time=   1.2s\n",
      "[CV 4/5; 18/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8\n",
      "[CV 4/5; 18/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8; total time=   2.2s\n",
      "[CV 5/5; 18/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8\n",
      "[CV 5/5; 18/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8; total time=   1.3s\n",
      "[CV 1/5; 19/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 19/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   2.5s\n",
      "[CV 2/5; 19/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 19/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   2.6s\n",
      "[CV 3/5; 19/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 19/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   3.6s\n",
      "[CV 4/5; 19/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 19/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.7s\n",
      "[CV 5/5; 19/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 19/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.8s\n",
      "[CV 1/5; 20/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 20/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.5s\n",
      "[CV 2/5; 20/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 20/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.9s\n",
      "[CV 3/5; 20/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 20/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 4/5; 20/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 20/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   2.2s\n",
      "[CV 5/5; 20/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 20/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.2s\n",
      "[CV 1/5; 21/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8\n",
      "[CV 1/5; 21/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8; total time=   2.0s\n",
      "[CV 2/5; 21/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 21/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8; total time=   1.8s\n",
      "[CV 3/5; 21/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8\n",
      "[CV 3/5; 21/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8; total time=   1.9s\n",
      "[CV 4/5; 21/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8\n",
      "[CV 4/5; 21/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8; total time=   1.5s\n",
      "[CV 5/5; 21/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8\n",
      "[CV 5/5; 21/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8; total time=   2.0s\n",
      "[CV 1/5; 22/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 22/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.7s\n",
      "[CV 2/5; 22/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 22/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.9s\n",
      "[CV 3/5; 22/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 22/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.8s\n",
      "[CV 4/5; 22/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 22/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.6s\n",
      "[CV 5/5; 22/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 22/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   2.0s\n",
      "[CV 1/5; 23/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 23/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.9s\n",
      "[CV 2/5; 23/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 23/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.9s\n",
      "[CV 3/5; 23/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 23/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.6s\n",
      "[CV 4/5; 23/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 23/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.8s\n",
      "[CV 5/5; 23/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 23/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.5s\n",
      "[CV 1/5; 24/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8\n",
      "[CV 1/5; 24/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8; total time=   1.7s\n",
      "[CV 2/5; 24/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8\n",
      "[CV 2/5; 24/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8; total time=   1.3s\n",
      "[CV 3/5; 24/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8\n",
      "[CV 3/5; 24/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8; total time=   2.2s\n",
      "[CV 4/5; 24/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8\n",
      "[CV 4/5; 24/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8; total time=   1.5s\n",
      "[CV 5/5; 24/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8\n",
      "[CV 5/5; 24/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8; total time=   1.9s\n",
      "[CV 1/5; 25/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2\n",
      "[CV 1/5; 25/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2; total time=   2.3s\n",
      "[CV 2/5; 25/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2\n",
      "[CV 2/5; 25/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2; total time=   2.1s\n",
      "[CV 3/5; 25/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2\n",
      "[CV 3/5; 25/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2; total time=   3.4s\n",
      "[CV 4/5; 25/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2\n",
      "[CV 4/5; 25/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2; total time=   3.5s\n",
      "[CV 5/5; 25/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2\n",
      "[CV 5/5; 25/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2; total time=   1.7s\n",
      "[CV 1/5; 26/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4\n",
      "[CV 1/5; 26/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4; total time=   2.0s\n",
      "[CV 2/5; 26/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4\n",
      "[CV 2/5; 26/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4; total time=   2.0s\n",
      "[CV 3/5; 26/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 26/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4; total time=   1.3s\n",
      "[CV 4/5; 26/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4\n",
      "[CV 4/5; 26/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4; total time=   1.9s\n",
      "[CV 5/5; 26/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4\n",
      "[CV 5/5; 26/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4; total time=   1.9s\n",
      "[CV 1/5; 27/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8\n",
      "[CV 1/5; 27/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8; total time=   1.7s\n",
      "[CV 2/5; 27/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8\n",
      "[CV 2/5; 27/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8; total time=   1.6s\n",
      "[CV 3/5; 27/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8\n",
      "[CV 3/5; 27/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8; total time=   1.8s\n",
      "[CV 4/5; 27/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8\n",
      "[CV 4/5; 27/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8; total time=   2.2s\n",
      "[CV 5/5; 27/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8\n",
      "[CV 5/5; 27/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8; total time=   1.3s\n",
      "[CV 1/5; 28/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 1/5; 28/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2; total time=   1.8s\n",
      "[CV 2/5; 28/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 2/5; 28/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2; total time=   2.0s\n",
      "[CV 3/5; 28/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 3/5; 28/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2; total time=   1.4s\n",
      "[CV 4/5; 28/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 4/5; 28/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2; total time=   1.7s\n",
      "[CV 5/5; 28/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 5/5; 28/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=2; total time=   2.1s\n",
      "[CV 1/5; 29/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 1/5; 29/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4; total time=   1.7s\n",
      "[CV 2/5; 29/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 2/5; 29/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4; total time=   1.4s\n",
      "[CV 3/5; 29/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 3/5; 29/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4; total time=   2.4s\n",
      "[CV 4/5; 29/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 4/5; 29/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4; total time=   2.0s\n",
      "[CV 5/5; 29/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 5/5; 29/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=4; total time=   1.5s\n",
      "[CV 1/5; 30/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 1/5; 30/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8; total time=   2.1s\n",
      "[CV 2/5; 30/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 2/5; 30/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8; total time=   2.2s\n",
      "[CV 3/5; 30/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 3/5; 30/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8; total time=   1.7s\n",
      "[CV 4/5; 30/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 4/5; 30/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8; total time=   1.7s\n",
      "[CV 5/5; 30/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 5/5; 30/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=4, neuron2=8; total time=   2.2s\n",
      "[CV 1/5; 31/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 1/5; 31/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2; total time=   2.3s\n",
      "[CV 2/5; 31/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 2/5; 31/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2; total time=   2.2s\n",
      "[CV 3/5; 31/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 3/5; 31/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2; total time=   4.2s\n",
      "[CV 4/5; 31/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 31/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2; total time=   1.9s\n",
      "[CV 5/5; 31/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 5/5; 31/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=2; total time=   1.7s\n",
      "[CV 1/5; 32/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=4\n",
      "[CV 1/5; 32/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=4; total time=   2.1s\n",
      "[CV 2/5; 32/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=4\n",
      "[CV 2/5; 32/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=4; total time=   2.0s\n",
      "[CV 3/5; 32/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=4\n",
      "[CV 3/5; 32/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=4; total time=   1.4s\n",
      "[CV 4/5; 32/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=4\n",
      "[CV 4/5; 32/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=4; total time=   2.0s\n",
      "[CV 5/5; 32/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=4\n",
      "[CV 5/5; 32/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=4; total time=   1.8s\n",
      "[CV 1/5; 33/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=8\n",
      "[CV 1/5; 33/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=8; total time=   1.4s\n",
      "[CV 2/5; 33/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=8\n",
      "[CV 2/5; 33/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=8; total time=   1.9s\n",
      "[CV 3/5; 33/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=8\n",
      "[CV 3/5; 33/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=8; total time=   2.3s\n",
      "[CV 4/5; 33/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=8\n",
      "[CV 4/5; 33/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=8; total time=   1.7s\n",
      "[CV 5/5; 33/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=8\n",
      "[CV 5/5; 33/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=8, neuron2=8; total time=   1.6s\n",
      "[CV 1/5; 34/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=2\n",
      "[CV 1/5; 34/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=2; total time=   2.1s\n",
      "[CV 2/5; 34/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=2\n",
      "[CV 2/5; 34/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=2; total time=   1.8s\n",
      "[CV 3/5; 34/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=2\n",
      "[CV 3/5; 34/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=2; total time=   1.3s\n",
      "[CV 4/5; 34/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=2\n",
      "[CV 4/5; 34/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=2; total time=   2.1s\n",
      "[CV 5/5; 34/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=2\n",
      "[CV 5/5; 34/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=2; total time=   1.8s\n",
      "[CV 1/5; 35/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=4\n",
      "[CV 1/5; 35/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=4; total time=   1.7s\n",
      "[CV 2/5; 35/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=4\n",
      "[CV 2/5; 35/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=4; total time=   1.7s\n",
      "[CV 3/5; 35/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=4\n",
      "[CV 3/5; 35/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=4; total time=   1.8s\n",
      "[CV 4/5; 35/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=4\n",
      "[CV 4/5; 35/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=4; total time=   2.1s\n",
      "[CV 5/5; 35/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=4\n",
      "[CV 5/5; 35/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=4; total time=   1.4s\n",
      "[CV 1/5; 36/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=8\n",
      "[CV 1/5; 36/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=8; total time=   2.2s\n",
      "[CV 2/5; 36/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=8\n",
      "[CV 2/5; 36/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=8; total time=   2.1s\n",
      "[CV 3/5; 36/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=8\n",
      "[CV 3/5; 36/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=8; total time=   1.8s\n",
      "[CV 4/5; 36/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=8\n",
      "[CV 4/5; 36/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=8; total time=   1.9s\n",
      "[CV 5/5; 36/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 36/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.001, neuron1=16, neuron2=8; total time=   2.0s\n",
      "[CV 1/5; 37/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 37/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   2.8s\n",
      "[CV 2/5; 37/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 37/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   3.5s\n",
      "[CV 3/5; 37/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 37/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   2.8s\n",
      "[CV 4/5; 37/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 37/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.4s\n",
      "[CV 5/5; 37/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 37/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.8s\n",
      "[CV 1/5; 38/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 38/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.7s\n",
      "[CV 2/5; 38/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 38/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.8s\n",
      "[CV 3/5; 38/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 38/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.6s\n",
      "[CV 4/5; 38/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 38/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   2.1s\n",
      "[CV 5/5; 38/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 38/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.8s\n",
      "[CV 1/5; 39/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=8\n",
      "[CV 1/5; 39/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=8; total time=   1.6s\n",
      "[CV 2/5; 39/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=8\n",
      "[CV 2/5; 39/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=8; total time=   1.8s\n",
      "[CV 3/5; 39/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=8\n",
      "[CV 3/5; 39/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=8; total time=   1.8s\n",
      "[CV 4/5; 39/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=8\n",
      "[CV 4/5; 39/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=8; total time=   1.7s\n",
      "[CV 5/5; 39/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=8\n",
      "[CV 5/5; 39/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=4, neuron2=8; total time=   1.6s\n",
      "[CV 1/5; 40/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 40/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.7s\n",
      "[CV 2/5; 40/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 40/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   2.1s\n",
      "[CV 3/5; 40/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 40/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.5s\n",
      "[CV 4/5; 40/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 40/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.8s\n",
      "[CV 5/5; 40/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 40/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.8s\n",
      "[CV 1/5; 41/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 41/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.9s\n",
      "[CV 2/5; 41/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 41/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.4s\n",
      "[CV 3/5; 41/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 41/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.8s\n",
      "[CV 4/5; 41/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 41/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   2.0s\n",
      "[CV 5/5; 41/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 41/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.3s\n",
      "[CV 1/5; 42/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 42/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=8; total time=   2.3s\n",
      "[CV 2/5; 42/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=8\n",
      "[CV 2/5; 42/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=8; total time=   2.1s\n",
      "[CV 3/5; 42/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=8\n",
      "[CV 3/5; 42/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=8; total time=   1.5s\n",
      "[CV 4/5; 42/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=8\n",
      "[CV 4/5; 42/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=8; total time=   1.7s\n",
      "[CV 5/5; 42/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=8\n",
      "[CV 5/5; 42/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=8, neuron2=8; total time=   2.2s\n",
      "[CV 1/5; 43/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=2\n",
      "[CV 1/5; 43/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=2; total time=   3.3s\n",
      "[CV 2/5; 43/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=2\n",
      "[CV 2/5; 43/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=2; total time=   3.3s\n",
      "[CV 3/5; 43/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=2\n",
      "[CV 3/5; 43/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=2; total time=   2.9s\n",
      "[CV 4/5; 43/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=2\n",
      "[CV 4/5; 43/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=2; total time=   1.6s\n",
      "[CV 5/5; 43/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=2\n",
      "[CV 5/5; 43/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=2; total time=   1.9s\n",
      "[CV 1/5; 44/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=4\n",
      "[CV 1/5; 44/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=4; total time=   1.9s\n",
      "[CV 2/5; 44/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=4\n",
      "[CV 2/5; 44/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=4; total time=   2.0s\n",
      "[CV 3/5; 44/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=4\n",
      "[CV 3/5; 44/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=4; total time=   1.7s\n",
      "[CV 4/5; 44/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=4\n",
      "[CV 4/5; 44/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=4; total time=   2.0s\n",
      "[CV 5/5; 44/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=4\n",
      "[CV 5/5; 44/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=4; total time=   2.5s\n",
      "[CV 1/5; 45/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=8\n",
      "[CV 1/5; 45/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=8; total time=   1.5s\n",
      "[CV 2/5; 45/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=8\n",
      "[CV 2/5; 45/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=8; total time=   2.1s\n",
      "[CV 3/5; 45/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=8\n",
      "[CV 3/5; 45/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=8; total time=   2.1s\n",
      "[CV 4/5; 45/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=8\n",
      "[CV 4/5; 45/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=8; total time=   1.4s\n",
      "[CV 5/5; 45/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=8\n",
      "[CV 5/5; 45/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.01, neuron1=16, neuron2=8; total time=   1.8s\n",
      "[CV 1/5; 46/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 46/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.9s\n",
      "[CV 2/5; 46/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 46/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   2.0s\n",
      "[CV 3/5; 46/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 46/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.8s\n",
      "[CV 4/5; 46/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 46/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.9s\n",
      "[CV 5/5; 46/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 46/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=2; total time=   2.0s\n",
      "[CV 1/5; 47/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 47/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.3s\n",
      "[CV 2/5; 47/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 47/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.7s\n",
      "[CV 3/5; 47/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 47/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   2.1s\n",
      "[CV 4/5; 47/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 47/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.6s\n",
      "[CV 5/5; 47/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 47/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.5s\n",
      "[CV 1/5; 48/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=8\n",
      "[CV 1/5; 48/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=8; total time=   2.4s\n",
      "[CV 2/5; 48/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=8\n",
      "[CV 2/5; 48/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=8; total time=   1.9s\n",
      "[CV 3/5; 48/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=8\n",
      "[CV 3/5; 48/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=8; total time=   1.2s\n",
      "[CV 4/5; 48/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=8\n",
      "[CV 4/5; 48/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=8; total time=   2.6s\n",
      "[CV 5/5; 48/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=8\n",
      "[CV 5/5; 48/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=4, neuron2=8; total time=   3.5s\n",
      "[CV 1/5; 49/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 49/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   2.4s\n",
      "[CV 2/5; 49/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 49/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   2.6s\n",
      "[CV 3/5; 49/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 49/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   2.4s\n",
      "[CV 4/5; 49/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 49/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 5/5; 49/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 49/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.8s\n",
      "[CV 1/5; 50/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 50/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   2.0s\n",
      "[CV 2/5; 50/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 50/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.9s\n",
      "[CV 3/5; 50/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 50/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.4s\n",
      "[CV 4/5; 50/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 50/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.9s\n",
      "[CV 5/5; 50/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 50/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.8s\n",
      "[CV 1/5; 51/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=8\n",
      "[CV 1/5; 51/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=8; total time=   1.4s\n",
      "[CV 2/5; 51/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=8\n",
      "[CV 2/5; 51/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=8; total time=   2.2s\n",
      "[CV 3/5; 51/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=8\n",
      "[CV 3/5; 51/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=8; total time=   1.8s\n",
      "[CV 4/5; 51/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=8\n",
      "[CV 4/5; 51/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=8; total time=   1.6s\n",
      "[CV 5/5; 51/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=8\n",
      "[CV 5/5; 51/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=8, neuron2=8; total time=   1.7s\n",
      "[CV 1/5; 52/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=2\n",
      "[CV 1/5; 52/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=2; total time=   1.9s\n",
      "[CV 2/5; 52/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=2\n",
      "[CV 2/5; 52/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=2; total time=   1.8s\n",
      "[CV 3/5; 52/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 52/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=2; total time=   1.3s\n",
      "[CV 4/5; 52/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=2\n",
      "[CV 4/5; 52/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=2; total time=   2.2s\n",
      "[CV 5/5; 52/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=2\n",
      "[CV 5/5; 52/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=2; total time=   2.3s\n",
      "[CV 1/5; 53/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=4\n",
      "[CV 1/5; 53/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=4; total time=   1.6s\n",
      "[CV 2/5; 53/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=4\n",
      "[CV 2/5; 53/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=4; total time=   1.9s\n",
      "[CV 3/5; 53/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=4\n",
      "[CV 3/5; 53/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=4; total time=   1.8s\n",
      "[CV 4/5; 53/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=4\n",
      "[CV 4/5; 53/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=4; total time=   1.7s\n",
      "[CV 5/5; 53/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=4\n",
      "[CV 5/5; 53/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=4; total time=   1.5s\n",
      "[CV 1/5; 54/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=8\n",
      "[CV 1/5; 54/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=8; total time=   1.8s\n",
      "[CV 2/5; 54/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=8\n",
      "[CV 2/5; 54/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=8; total time=   1.8s\n",
      "[CV 3/5; 54/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=8\n",
      "[CV 3/5; 54/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=8; total time=   1.9s\n",
      "[CV 4/5; 54/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=8\n",
      "[CV 4/5; 54/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=8; total time=   1.9s\n",
      "[CV 5/5; 54/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=8\n",
      "[CV 5/5; 54/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=normal, learning_rate=0.1, neuron1=16, neuron2=8; total time=   1.8s\n",
      "[CV 1/5; 55/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 1/5; 55/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=2; total time=   1.8s\n",
      "[CV 2/5; 55/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 2/5; 55/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=2; total time=   1.5s\n",
      "[CV 3/5; 55/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 3/5; 55/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=2; total time=   1.8s\n",
      "[CV 4/5; 55/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 4/5; 55/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=2; total time=   2.0s\n",
      "[CV 5/5; 55/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 5/5; 55/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=2; total time=   1.6s\n",
      "[CV 1/5; 56/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 1/5; 56/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=4; total time=   2.1s\n",
      "[CV 2/5; 56/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 2/5; 56/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=4; total time=   2.1s\n",
      "[CV 3/5; 56/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 3/5; 56/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=4; total time=   1.7s\n",
      "[CV 4/5; 56/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 4/5; 56/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=4; total time=   1.7s\n",
      "[CV 5/5; 56/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 5/5; 56/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=4; total time=   1.9s\n",
      "[CV 1/5; 57/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 1/5; 57/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=8; total time=   1.9s\n",
      "[CV 2/5; 57/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 2/5; 57/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=8; total time=   1.3s\n",
      "[CV 3/5; 57/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 3/5; 57/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=8; total time=   2.0s\n",
      "[CV 4/5; 57/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 57/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=8; total time=   2.0s\n",
      "[CV 5/5; 57/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 5/5; 57/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=4, neuron2=8; total time=   1.6s\n",
      "[CV 1/5; 58/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 1/5; 58/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=2; total time=   1.9s\n",
      "[CV 2/5; 58/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 2/5; 58/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=2; total time=   2.1s\n",
      "[CV 3/5; 58/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 3/5; 58/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=2; total time=   1.8s\n",
      "[CV 4/5; 58/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 4/5; 58/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=2; total time=   2.5s\n",
      "[CV 5/5; 58/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 5/5; 58/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=2; total time=   3.6s\n",
      "[CV 1/5; 59/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=4\n",
      "[CV 1/5; 59/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=4; total time=   2.1s\n",
      "[CV 2/5; 59/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=4\n",
      "[CV 2/5; 59/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=4; total time=   2.1s\n",
      "[CV 3/5; 59/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=4\n",
      "[CV 3/5; 59/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=4; total time=   1.8s\n",
      "[CV 4/5; 59/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=4\n",
      "[CV 4/5; 59/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=4; total time=   2.1s\n",
      "[CV 5/5; 59/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=4\n",
      "[CV 5/5; 59/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=4; total time=   1.8s\n",
      "[CV 1/5; 60/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=8\n",
      "[CV 1/5; 60/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=8; total time=   1.8s\n",
      "[CV 2/5; 60/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=8\n",
      "[CV 2/5; 60/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=8; total time=   1.9s\n",
      "[CV 3/5; 60/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=8\n",
      "[CV 3/5; 60/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=8; total time=   1.5s\n",
      "[CV 4/5; 60/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=8\n",
      "[CV 4/5; 60/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=8; total time=   3.3s\n",
      "[CV 5/5; 60/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=8\n",
      "[CV 5/5; 60/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=8, neuron2=8; total time=   3.8s\n",
      "[CV 1/5; 61/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=2\n",
      "[CV 1/5; 61/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=2; total time=   2.2s\n",
      "[CV 2/5; 61/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=2\n",
      "[CV 2/5; 61/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=2; total time=   1.9s\n",
      "[CV 3/5; 61/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=2\n",
      "[CV 3/5; 61/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=2; total time=   2.0s\n",
      "[CV 4/5; 61/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=2\n",
      "[CV 4/5; 61/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=2; total time=   1.4s\n",
      "[CV 5/5; 61/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=2\n",
      "[CV 5/5; 61/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=2; total time=   2.1s\n",
      "[CV 1/5; 62/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=4\n",
      "[CV 1/5; 62/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=4; total time=   2.4s\n",
      "[CV 2/5; 62/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=4\n",
      "[CV 2/5; 62/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=4; total time=   1.5s\n",
      "[CV 3/5; 62/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=4\n",
      "[CV 3/5; 62/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=4; total time=   1.9s\n",
      "[CV 4/5; 62/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=4\n",
      "[CV 4/5; 62/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=4; total time=   1.8s\n",
      "[CV 5/5; 62/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 62/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=4; total time=   1.9s\n",
      "[CV 1/5; 63/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=8\n",
      "[CV 1/5; 63/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=8; total time=   1.5s\n",
      "[CV 2/5; 63/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=8\n",
      "[CV 2/5; 63/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=8; total time=   1.9s\n",
      "[CV 3/5; 63/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=8\n",
      "[CV 3/5; 63/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=8; total time=   2.1s\n",
      "[CV 4/5; 63/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=8\n",
      "[CV 4/5; 63/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=8; total time=   1.4s\n",
      "[CV 5/5; 63/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=8\n",
      "[CV 5/5; 63/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.001, neuron1=16, neuron2=8; total time=   2.0s\n",
      "[CV 1/5; 64/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 64/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.9s\n",
      "[CV 2/5; 64/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 64/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.4s\n",
      "[CV 3/5; 64/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 64/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.7s\n",
      "[CV 4/5; 64/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 64/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.9s\n",
      "[CV 5/5; 64/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 64/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=2; total time=   1.6s\n",
      "[CV 1/5; 65/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 65/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.2s\n",
      "[CV 2/5; 65/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 65/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.9s\n",
      "[CV 3/5; 65/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 65/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=4; total time=   2.3s\n",
      "[CV 4/5; 65/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 65/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.5s\n",
      "[CV 5/5; 65/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 65/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=4; total time=   1.5s\n",
      "[CV 1/5; 66/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=8\n",
      "[CV 1/5; 66/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=8; total time=   2.0s\n",
      "[CV 2/5; 66/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=8\n",
      "[CV 2/5; 66/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=8; total time=   1.7s\n",
      "[CV 3/5; 66/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=8\n",
      "[CV 3/5; 66/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=8; total time=   1.5s\n",
      "[CV 4/5; 66/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=8\n",
      "[CV 4/5; 66/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=8; total time=   3.8s\n",
      "[CV 5/5; 66/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=8\n",
      "[CV 5/5; 66/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=4, neuron2=8; total time=   3.2s\n",
      "[CV 1/5; 67/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 67/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.9s\n",
      "[CV 2/5; 67/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 67/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=2; total time=   2.1s\n",
      "[CV 3/5; 67/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 67/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.7s\n",
      "[CV 4/5; 67/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 67/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=2; total time=   1.3s\n",
      "[CV 5/5; 67/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 67/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=2; total time=   2.1s\n",
      "[CV 1/5; 68/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 68/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.8s\n",
      "[CV 2/5; 68/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 68/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.5s\n",
      "[CV 3/5; 68/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 68/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=4; total time=   2.0s\n",
      "[CV 4/5; 68/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 68/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=4; total time=   2.4s\n",
      "[CV 5/5; 68/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 68/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=4; total time=   1.8s\n",
      "[CV 1/5; 69/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=8\n",
      "[CV 1/5; 69/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=8; total time=   1.5s\n",
      "[CV 2/5; 69/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=8\n",
      "[CV 2/5; 69/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=8; total time=   2.1s\n",
      "[CV 3/5; 69/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=8\n",
      "[CV 3/5; 69/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=8; total time=   1.8s\n",
      "[CV 4/5; 69/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=8\n",
      "[CV 4/5; 69/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=8; total time=   1.3s\n",
      "[CV 5/5; 69/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=8\n",
      "[CV 5/5; 69/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=8, neuron2=8; total time=   2.1s\n",
      "[CV 1/5; 70/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=2\n",
      "[CV 1/5; 70/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=2; total time=   2.0s\n",
      "[CV 2/5; 70/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=2\n",
      "[CV 2/5; 70/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=2; total time=   1.7s\n",
      "[CV 3/5; 70/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=2\n",
      "[CV 3/5; 70/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=2; total time=   1.9s\n",
      "[CV 4/5; 70/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=2\n",
      "[CV 4/5; 70/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=2; total time=   1.7s\n",
      "[CV 5/5; 70/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=2\n",
      "[CV 5/5; 70/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=2; total time=   1.7s\n",
      "[CV 1/5; 71/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=4\n",
      "[CV 1/5; 71/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=4; total time=   1.6s\n",
      "[CV 2/5; 71/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=4\n",
      "[CV 2/5; 71/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=4; total time=   1.9s\n",
      "[CV 3/5; 71/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=4\n",
      "[CV 3/5; 71/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=4; total time=   1.8s\n",
      "[CV 4/5; 71/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=4\n",
      "[CV 4/5; 71/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=4; total time=   1.4s\n",
      "[CV 5/5; 71/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=4\n",
      "[CV 5/5; 71/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=4; total time=   2.3s\n",
      "[CV 1/5; 72/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=8\n",
      "[CV 1/5; 72/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=8; total time=   2.0s\n",
      "[CV 2/5; 72/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=8\n",
      "[CV 2/5; 72/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=8; total time=   1.8s\n",
      "[CV 3/5; 72/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=8\n",
      "[CV 3/5; 72/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=8; total time=   2.6s\n",
      "[CV 4/5; 72/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=8\n",
      "[CV 4/5; 72/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=8; total time=   3.7s\n",
      "[CV 5/5; 72/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=8\n",
      "[CV 5/5; 72/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.01, neuron1=16, neuron2=8; total time=   2.1s\n",
      "[CV 1/5; 73/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 73/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=2; total time=   2.6s\n",
      "[CV 2/5; 73/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 73/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=2; total time=   2.0s\n",
      "[CV 3/5; 73/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 73/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.4s\n",
      "[CV 4/5; 73/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 73/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.8s\n",
      "[CV 5/5; 73/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 73/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=2; total time=   1.8s\n",
      "[CV 1/5; 74/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 74/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.7s\n",
      "[CV 2/5; 74/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 74/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.6s\n",
      "[CV 3/5; 74/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 74/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.8s\n",
      "[CV 4/5; 74/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 74/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.8s\n",
      "[CV 5/5; 74/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 74/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=4; total time=   1.8s\n",
      "[CV 1/5; 75/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=8\n",
      "[CV 1/5; 75/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=8; total time=   2.3s\n",
      "[CV 2/5; 75/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=8\n",
      "[CV 2/5; 75/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=8; total time=   2.0s\n",
      "[CV 3/5; 75/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=8\n",
      "[CV 3/5; 75/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=8; total time=   1.6s\n",
      "[CV 4/5; 75/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=8\n",
      "[CV 4/5; 75/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=8; total time=   1.7s\n",
      "[CV 5/5; 75/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=8\n",
      "[CV 5/5; 75/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=4, neuron2=8; total time=   1.9s\n",
      "[CV 1/5; 76/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 76/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.9s\n",
      "[CV 2/5; 76/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 76/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.5s\n",
      "[CV 3/5; 76/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 76/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=2; total time=   2.2s\n",
      "[CV 4/5; 76/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 76/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.8s\n",
      "[CV 5/5; 76/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 76/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=2; total time=   1.7s\n",
      "[CV 1/5; 77/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 77/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.7s\n",
      "[CV 2/5; 77/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 77/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.9s\n",
      "[CV 3/5; 77/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 77/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.8s\n",
      "[CV 4/5; 77/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 4/5; 77/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.3s\n",
      "[CV 5/5; 77/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 77/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=4; total time=   1.7s\n",
      "[CV 1/5; 78/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=8\n",
      "[CV 1/5; 78/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=8; total time=   1.7s\n",
      "[CV 2/5; 78/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=8\n",
      "[CV 2/5; 78/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=8; total time=   2.2s\n",
      "[CV 3/5; 78/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=8\n",
      "[CV 3/5; 78/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=8; total time=   2.9s\n",
      "[CV 4/5; 78/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 78/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=8; total time=   3.8s\n",
      "[CV 5/5; 78/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=8\n",
      "[CV 5/5; 78/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=8, neuron2=8; total time=   2.0s\n",
      "[CV 1/5; 79/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=2\n",
      "[CV 1/5; 79/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=2; total time=   1.9s\n",
      "[CV 2/5; 79/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=2\n",
      "[CV 2/5; 79/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=2; total time=   1.9s\n",
      "[CV 3/5; 79/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=2\n",
      "[CV 3/5; 79/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=2; total time=   1.9s\n",
      "[CV 4/5; 79/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=2\n",
      "[CV 4/5; 79/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=2; total time=   1.7s\n",
      "[CV 5/5; 79/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=2\n",
      "[CV 5/5; 79/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=2; total time=   1.7s\n",
      "[CV 1/5; 80/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=4\n",
      "[CV 1/5; 80/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=4; total time=   1.9s\n",
      "[CV 2/5; 80/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=4\n",
      "[CV 2/5; 80/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=4; total time=   1.3s\n",
      "[CV 3/5; 80/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=4\n",
      "[CV 3/5; 80/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=4; total time=   1.9s\n",
      "[CV 4/5; 80/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=4\n",
      "[CV 4/5; 80/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=4; total time=   2.1s\n",
      "[CV 5/5; 80/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=4\n",
      "[CV 5/5; 80/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=4; total time=   1.5s\n",
      "[CV 1/5; 81/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=8\n",
      "[CV 1/5; 81/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=8; total time=   1.7s\n",
      "[CV 2/5; 81/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=8\n",
      "[CV 2/5; 81/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=8; total time=   2.4s\n",
      "[CV 3/5; 81/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=8\n",
      "[CV 3/5; 81/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=8; total time=   2.5s\n",
      "[CV 4/5; 81/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=8\n",
      "[CV 4/5; 81/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=8; total time=   1.9s\n",
      "[CV 5/5; 81/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=8\n",
      "[CV 5/5; 81/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=10, init=zero, learning_rate=0.1, neuron1=16, neuron2=8; total time=   2.1s\n",
      "[CV 1/5; 82/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 1/5; 82/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2; total time=   3.6s\n",
      "[CV 2/5; 82/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 2/5; 82/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2; total time=   3.1s\n",
      "[CV 3/5; 82/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 3/5; 82/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2; total time=   3.4s\n",
      "[CV 4/5; 82/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 4/5; 82/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2; total time=   3.3s\n",
      "[CV 5/5; 82/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 5/5; 82/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=2; total time=   3.1s\n",
      "[CV 1/5; 83/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 1/5; 83/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4; total time=   3.1s\n",
      "[CV 2/5; 83/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 2/5; 83/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4; total time=   4.7s\n",
      "[CV 3/5; 83/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 3/5; 83/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4; total time=   5.2s\n",
      "[CV 4/5; 83/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 4/5; 83/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4; total time=   3.2s\n",
      "[CV 5/5; 83/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 83/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=4; total time=   3.6s\n",
      "[CV 1/5; 84/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 1/5; 84/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8; total time=   3.5s\n",
      "[CV 2/5; 84/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 2/5; 84/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8; total time=   3.4s\n",
      "[CV 3/5; 84/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 3/5; 84/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8; total time=   3.5s\n",
      "[CV 4/5; 84/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 4/5; 84/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8; total time=   3.4s\n",
      "[CV 5/5; 84/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 5/5; 84/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=4, neuron2=8; total time=   3.1s\n",
      "[CV 1/5; 85/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 1/5; 85/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2; total time=   3.7s\n",
      "[CV 2/5; 85/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 2/5; 85/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2; total time=   3.7s\n",
      "[CV 3/5; 85/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 3/5; 85/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2; total time=   3.4s\n",
      "[CV 4/5; 85/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 4/5; 85/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2; total time=   3.2s\n",
      "[CV 5/5; 85/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 5/5; 85/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=2; total time=   4.2s\n",
      "[CV 1/5; 86/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n",
      "[CV 1/5; 86/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4; total time=   3.0s\n",
      "[CV 2/5; 86/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n",
      "[CV 2/5; 86/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4; total time=   3.4s\n",
      "[CV 3/5; 86/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n",
      "[CV 3/5; 86/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4; total time=   3.7s\n",
      "[CV 4/5; 86/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n",
      "[CV 4/5; 86/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4; total time=   6.3s\n",
      "[CV 5/5; 86/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4\n",
      "[CV 5/5; 86/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=4; total time=   3.2s\n",
      "[CV 1/5; 87/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n",
      "[CV 1/5; 87/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8; total time=   3.2s\n",
      "[CV 2/5; 87/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n",
      "[CV 2/5; 87/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8; total time=   3.2s\n",
      "[CV 3/5; 87/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n",
      "[CV 3/5; 87/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8; total time=   3.4s\n",
      "[CV 4/5; 87/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n",
      "[CV 4/5; 87/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8; total time=   3.3s\n",
      "[CV 5/5; 87/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8\n",
      "[CV 5/5; 87/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=8, neuron2=8; total time=   3.8s\n",
      "[CV 1/5; 88/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2\n",
      "[CV 1/5; 88/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2; total time=   3.1s\n",
      "[CV 2/5; 88/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2\n",
      "[CV 2/5; 88/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2; total time=   3.5s\n",
      "[CV 3/5; 88/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2\n",
      "[CV 3/5; 88/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2; total time=   3.7s\n",
      "[CV 4/5; 88/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2\n",
      "[CV 4/5; 88/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2; total time=   3.5s\n",
      "[CV 5/5; 88/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2\n",
      "[CV 5/5; 88/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=2; total time=   3.5s\n",
      "[CV 1/5; 89/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 89/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4; total time=   3.8s\n",
      "[CV 2/5; 89/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4\n",
      "[CV 2/5; 89/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4; total time=   3.5s\n",
      "[CV 3/5; 89/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4\n",
      "[CV 3/5; 89/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4; total time=   3.5s\n",
      "[CV 4/5; 89/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4\n",
      "[CV 4/5; 89/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4; total time=   4.5s\n",
      "[CV 5/5; 89/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4\n",
      "[CV 5/5; 89/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=4; total time=   5.8s\n",
      "[CV 1/5; 90/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8\n",
      "[CV 1/5; 90/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8; total time=   3.2s\n",
      "[CV 2/5; 90/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8\n",
      "[CV 2/5; 90/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8; total time=   3.7s\n",
      "[CV 3/5; 90/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8\n",
      "[CV 3/5; 90/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8; total time=   2.8s\n",
      "[CV 4/5; 90/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8\n",
      "[CV 4/5; 90/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8; total time=   3.6s\n",
      "[CV 5/5; 90/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8\n",
      "[CV 5/5; 90/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.001, neuron1=16, neuron2=8; total time=   3.1s\n",
      "[CV 1/5; 91/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 91/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   3.9s\n",
      "[CV 2/5; 91/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 91/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   3.5s\n",
      "[CV 3/5; 91/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 91/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   3.5s\n",
      "[CV 4/5; 91/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 91/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   3.0s\n",
      "[CV 5/5; 91/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 91/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=2; total time=   4.1s\n",
      "[CV 1/5; 92/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 92/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   3.4s\n",
      "[CV 2/5; 92/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 92/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   3.8s\n",
      "[CV 3/5; 92/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 92/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   3.5s\n",
      "[CV 4/5; 92/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 92/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   3.7s\n",
      "[CV 5/5; 92/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 92/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=4; total time=   4.1s\n",
      "[CV 1/5; 93/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8\n",
      "[CV 1/5; 93/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8; total time=   5.5s\n",
      "[CV 2/5; 93/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8\n",
      "[CV 2/5; 93/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8; total time=   3.1s\n",
      "[CV 3/5; 93/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8\n",
      "[CV 3/5; 93/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8; total time=   3.4s\n",
      "[CV 4/5; 93/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8\n",
      "[CV 4/5; 93/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8; total time=   3.3s\n",
      "[CV 5/5; 93/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8\n",
      "[CV 5/5; 93/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=4, neuron2=8; total time=   3.1s\n",
      "[CV 1/5; 94/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 94/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   3.0s\n",
      "[CV 2/5; 94/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 94/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   3.5s\n",
      "[CV 3/5; 94/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 94/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   3.4s\n",
      "[CV 4/5; 94/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 94/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   3.2s\n",
      "[CV 5/5; 94/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 94/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=2; total time=   3.3s\n",
      "[CV 1/5; 95/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 95/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   3.1s\n",
      "[CV 2/5; 95/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 95/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   3.1s\n",
      "[CV 3/5; 95/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 95/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   3.6s\n",
      "[CV 4/5; 95/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 95/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   3.1s\n",
      "[CV 5/5; 95/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 95/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=4; total time=   3.3s\n",
      "[CV 1/5; 96/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8\n",
      "[CV 1/5; 96/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8; total time=   3.6s\n",
      "[CV 2/5; 96/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8\n",
      "[CV 2/5; 96/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8; total time=   5.2s\n",
      "[CV 3/5; 96/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8\n",
      "[CV 3/5; 96/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8; total time=   5.5s\n",
      "[CV 4/5; 96/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8\n",
      "[CV 4/5; 96/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8; total time=   3.6s\n",
      "[CV 5/5; 96/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8\n",
      "[CV 5/5; 96/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=8, neuron2=8; total time=   4.2s\n",
      "[CV 1/5; 97/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2\n",
      "[CV 1/5; 97/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2; total time=   3.9s\n",
      "[CV 2/5; 97/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2\n",
      "[CV 2/5; 97/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2; total time=   4.1s\n",
      "[CV 3/5; 97/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2\n",
      "[CV 3/5; 97/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2; total time=   3.9s\n",
      "[CV 4/5; 97/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2\n",
      "[CV 4/5; 97/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2; total time=   4.6s\n",
      "[CV 5/5; 97/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2\n",
      "[CV 5/5; 97/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=2; total time=   3.8s\n",
      "[CV 1/5; 98/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4\n",
      "[CV 1/5; 98/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4; total time=   5.9s\n",
      "[CV 2/5; 98/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4\n",
      "[CV 2/5; 98/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4; total time=   3.3s\n",
      "[CV 3/5; 98/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4\n",
      "[CV 3/5; 98/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4; total time=   4.0s\n",
      "[CV 4/5; 98/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4\n",
      "[CV 4/5; 98/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4; total time=   3.6s\n",
      "[CV 5/5; 98/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4\n",
      "[CV 5/5; 98/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=4; total time=   4.6s\n",
      "[CV 1/5; 99/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8\n",
      "[CV 1/5; 99/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8; total time=   5.3s\n",
      "[CV 2/5; 99/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8\n",
      "[CV 2/5; 99/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8; total time=   5.3s\n",
      "[CV 3/5; 99/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 99/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8; total time=   4.4s\n",
      "[CV 4/5; 99/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8\n",
      "[CV 4/5; 99/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8; total time=   3.9s\n",
      "[CV 5/5; 99/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8\n",
      "[CV 5/5; 99/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.01, neuron1=16, neuron2=8; total time=   3.4s\n",
      "[CV 1/5; 100/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 1/5; 100/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   3.8s\n",
      "[CV 2/5; 100/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 2/5; 100/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   3.5s\n",
      "[CV 3/5; 100/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 3/5; 100/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   3.2s\n",
      "[CV 4/5; 100/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 4/5; 100/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   3.4s\n",
      "[CV 5/5; 100/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2\n",
      "[CV 5/5; 100/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=2; total time=   3.4s\n",
      "[CV 1/5; 101/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 1/5; 101/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   2.8s\n",
      "[CV 2/5; 101/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 2/5; 101/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   3.6s\n",
      "[CV 3/5; 101/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 3/5; 101/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   3.9s\n",
      "[CV 4/5; 101/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 4/5; 101/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   4.3s\n",
      "[CV 5/5; 101/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4\n",
      "[CV 5/5; 101/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=4; total time=   4.2s\n",
      "[CV 1/5; 102/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8\n",
      "[CV 1/5; 102/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8; total time=   6.3s\n",
      "[CV 2/5; 102/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8\n",
      "[CV 2/5; 102/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8; total time=   4.9s\n",
      "[CV 3/5; 102/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8\n",
      "[CV 3/5; 102/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8; total time=   3.4s\n",
      "[CV 4/5; 102/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8\n",
      "[CV 4/5; 102/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8; total time=   3.7s\n",
      "[CV 5/5; 102/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8\n",
      "[CV 5/5; 102/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=4, neuron2=8; total time=   3.7s\n",
      "[CV 1/5; 103/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 1/5; 103/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   4.1s\n",
      "[CV 2/5; 103/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 2/5; 103/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   4.0s\n",
      "[CV 3/5; 103/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 3/5; 103/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   5.7s\n",
      "[CV 4/5; 103/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 4/5; 103/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   3.9s\n",
      "[CV 5/5; 103/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2\n",
      "[CV 5/5; 103/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=2; total time=   4.3s\n",
      "[CV 1/5; 104/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 1/5; 104/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   3.6s\n",
      "[CV 2/5; 104/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 2/5; 104/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   3.8s\n",
      "[CV 3/5; 104/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 3/5; 104/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   3.5s\n",
      "[CV 4/5; 104/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 104/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   6.4s\n",
      "[CV 5/5; 104/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4\n",
      "[CV 5/5; 104/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=4; total time=   6.2s\n",
      "[CV 1/5; 105/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8\n",
      "[CV 1/5; 105/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8; total time=   5.7s\n",
      "[CV 2/5; 105/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8\n",
      "[CV 2/5; 105/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8; total time=   3.4s\n",
      "[CV 3/5; 105/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8\n",
      "[CV 3/5; 105/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8; total time=   3.3s\n",
      "[CV 4/5; 105/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8\n",
      "[CV 4/5; 105/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8; total time=   3.3s\n",
      "[CV 5/5; 105/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8\n",
      "[CV 5/5; 105/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=8, neuron2=8; total time=   4.5s\n",
      "[CV 1/5; 106/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2\n",
      "[CV 1/5; 106/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2; total time=   3.3s\n",
      "[CV 2/5; 106/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2\n",
      "[CV 2/5; 106/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2; total time=   3.4s\n",
      "[CV 3/5; 106/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2\n",
      "[CV 3/5; 106/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2; total time=   3.1s\n",
      "[CV 4/5; 106/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2\n",
      "[CV 4/5; 106/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2; total time=   3.4s\n",
      "[CV 5/5; 106/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2\n",
      "[CV 5/5; 106/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=2; total time=   3.6s\n",
      "[CV 1/5; 107/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4\n",
      "[CV 1/5; 107/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4; total time=   4.5s\n",
      "[CV 2/5; 107/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4\n",
      "[CV 2/5; 107/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4; total time=   3.6s\n",
      "[CV 3/5; 107/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4\n",
      "[CV 3/5; 107/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4; total time=   4.7s\n",
      "[CV 4/5; 107/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4\n",
      "[CV 4/5; 107/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4; total time=   6.8s\n",
      "[CV 5/5; 107/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4\n",
      "[CV 5/5; 107/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=4; total time=   5.1s\n",
      "[CV 1/5; 108/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8\n",
      "[CV 1/5; 108/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8; total time=   3.6s\n",
      "[CV 2/5; 108/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8\n",
      "[CV 2/5; 108/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8; total time=   4.2s\n",
      "[CV 3/5; 108/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8\n",
      "[CV 3/5; 108/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8; total time=   4.4s\n",
      "[CV 4/5; 108/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8\n",
      "[CV 4/5; 108/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8; total time=   3.3s\n",
      "[CV 5/5; 108/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8\n",
      "[CV 5/5; 108/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=uniform, learning_rate=0.1, neuron1=16, neuron2=8; total time=   3.5s\n",
      "[CV 1/5; 109/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 1/5; 109/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=2; total time=   3.8s\n",
      "[CV 2/5; 109/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 2/5; 109/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=2; total time=   3.4s\n",
      "[CV 3/5; 109/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 3/5; 109/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=2; total time=   3.5s\n",
      "[CV 4/5; 109/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=2\n",
      "[CV 4/5; 109/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=2; total time=   3.6s\n",
      "[CV 5/5; 109/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 109/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=2; total time=   3.7s\n",
      "[CV 1/5; 110/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 1/5; 110/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=4; total time=   4.0s\n",
      "[CV 2/5; 110/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 2/5; 110/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=4; total time=   3.4s\n",
      "[CV 3/5; 110/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 3/5; 110/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=4; total time=   6.9s\n",
      "[CV 4/5; 110/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 4/5; 110/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=4; total time=   4.2s\n",
      "[CV 5/5; 110/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=4\n",
      "[CV 5/5; 110/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=4; total time=   3.2s\n",
      "[CV 1/5; 111/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 1/5; 111/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=8; total time=   3.3s\n",
      "[CV 2/5; 111/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 2/5; 111/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=8; total time=   4.5s\n",
      "[CV 3/5; 111/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 3/5; 111/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=8; total time=   4.1s\n",
      "[CV 4/5; 111/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 4/5; 111/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=8; total time=   4.8s\n",
      "[CV 5/5; 111/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=8\n",
      "[CV 5/5; 111/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=4, neuron2=8; total time=   3.3s\n",
      "[CV 1/5; 112/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 1/5; 112/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=2; total time=   4.0s\n",
      "[CV 2/5; 112/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 2/5; 112/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=2; total time=   3.8s\n",
      "[CV 3/5; 112/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 3/5; 112/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=2; total time=   4.1s\n",
      "[CV 4/5; 112/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 4/5; 112/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=2; total time=   3.6s\n",
      "[CV 5/5; 112/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=2\n",
      "[CV 5/5; 112/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=2; total time=   4.1s\n",
      "[CV 1/5; 113/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=4\n",
      "[CV 1/5; 113/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=4; total time=   3.3s\n",
      "[CV 2/5; 113/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=4\n",
      "[CV 2/5; 113/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=4; total time=   4.6s\n",
      "[CV 3/5; 113/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=4\n",
      "[CV 3/5; 113/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=4; total time=   5.4s\n",
      "[CV 4/5; 113/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=4\n",
      "[CV 4/5; 113/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=4; total time=   4.5s\n",
      "[CV 5/5; 113/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=4\n",
      "[CV 5/5; 113/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=4; total time=   3.1s\n",
      "[CV 1/5; 114/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=8\n",
      "[CV 1/5; 114/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=8; total time=   3.4s\n",
      "[CV 2/5; 114/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=8\n",
      "[CV 2/5; 114/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=8; total time=   3.3s\n",
      "[CV 3/5; 114/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=8\n",
      "[CV 3/5; 114/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=8; total time=   3.4s\n",
      "[CV 4/5; 114/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=8\n",
      "[CV 4/5; 114/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=8; total time=   3.6s\n",
      "[CV 5/5; 114/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=8\n",
      "[CV 5/5; 114/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=8, neuron2=8; total time=   4.1s\n",
      "[CV 1/5; 115/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 115/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=2; total time=   3.5s\n",
      "[CV 2/5; 115/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=2\n",
      "[CV 2/5; 115/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=2; total time=   4.2s\n",
      "[CV 3/5; 115/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=2\n",
      "[CV 3/5; 115/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=2; total time=   3.7s\n",
      "[CV 4/5; 115/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=2\n",
      "[CV 4/5; 115/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=2; total time=   4.4s\n",
      "[CV 5/5; 115/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=2\n",
      "[CV 5/5; 115/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=2; total time=   3.6s\n",
      "[CV 1/5; 116/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=4\n",
      "[CV 1/5; 116/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=4; total time=   3.8s\n",
      "[CV 2/5; 116/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=4\n",
      "[CV 2/5; 116/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=4; total time=   5.1s\n",
      "[CV 3/5; 116/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=4\n",
      "[CV 3/5; 116/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=4; total time=   6.6s\n",
      "[CV 4/5; 116/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=4\n",
      "[CV 4/5; 116/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=4; total time=   3.3s\n",
      "[CV 5/5; 116/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=4\n",
      "[CV 5/5; 116/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=4; total time=   4.3s\n",
      "[CV 1/5; 117/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=8\n",
      "[CV 1/5; 117/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=8; total time=   3.4s\n",
      "[CV 2/5; 117/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=8\n",
      "[CV 2/5; 117/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=8; total time=   3.6s\n",
      "[CV 3/5; 117/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=8\n",
      "[CV 3/5; 117/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=8; total time=   3.8s\n",
      "[CV 4/5; 117/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=8\n",
      "[CV 4/5; 117/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=8; total time=   4.0s\n",
      "[CV 5/5; 117/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=8\n",
      "[CV 5/5; 117/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.001, neuron1=16, neuron2=8; total time=   3.1s\n",
      "[CV 1/5; 118/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 1/5; 118/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   4.4s\n",
      "[CV 2/5; 118/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 2/5; 118/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   3.3s\n",
      "[CV 3/5; 118/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 3/5; 118/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   3.9s\n",
      "[CV 4/5; 118/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 4/5; 118/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   3.1s\n",
      "[CV 5/5; 118/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2\n",
      "[CV 5/5; 118/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=2; total time=   3.6s\n",
      "[CV 1/5; 119/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 1/5; 119/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   3.2s\n",
      "[CV 2/5; 119/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 2/5; 119/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   5.4s\n",
      "[CV 3/5; 119/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 3/5; 119/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   4.8s\n",
      "[CV 4/5; 119/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 4/5; 119/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   3.6s\n",
      "[CV 5/5; 119/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4\n",
      "[CV 5/5; 119/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=4; total time=   3.2s\n",
      "[CV 1/5; 120/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=8\n",
      "[CV 1/5; 120/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=8; total time=   4.0s\n",
      "[CV 2/5; 120/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 120/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=8; total time=   2.9s\n",
      "[CV 3/5; 120/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=8\n",
      "[CV 3/5; 120/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=8; total time=   3.7s\n",
      "[CV 4/5; 120/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=8\n",
      "[CV 4/5; 120/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=8; total time=   3.4s\n",
      "[CV 5/5; 120/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=8\n",
      "[CV 5/5; 120/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=4, neuron2=8; total time=   3.4s\n",
      "[CV 1/5; 121/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 1/5; 121/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   3.6s\n",
      "[CV 2/5; 121/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 2/5; 121/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   3.3s\n",
      "[CV 3/5; 121/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 3/5; 121/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   3.4s\n",
      "[CV 4/5; 121/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 4/5; 121/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   3.4s\n",
      "[CV 5/5; 121/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2\n",
      "[CV 5/5; 121/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=2; total time=   3.4s\n",
      "[CV 1/5; 122/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 1/5; 122/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   3.6s\n",
      "[CV 2/5; 122/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 2/5; 122/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   2.8s\n",
      "[CV 3/5; 122/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 3/5; 122/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   4.2s\n",
      "[CV 4/5; 122/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 4/5; 122/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   5.0s\n",
      "[CV 5/5; 122/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4\n",
      "[CV 5/5; 122/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=4; total time=   4.1s\n",
      "[CV 1/5; 123/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=8\n",
      "[CV 1/5; 123/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=8; total time=   3.2s\n",
      "[CV 2/5; 123/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=8\n",
      "[CV 2/5; 123/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=8; total time=   3.9s\n",
      "[CV 3/5; 123/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=8\n",
      "[CV 3/5; 123/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=8; total time=   3.6s\n",
      "[CV 4/5; 123/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=8\n",
      "[CV 4/5; 123/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=8; total time=   3.8s\n",
      "[CV 5/5; 123/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=8\n",
      "[CV 5/5; 123/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=8, neuron2=8; total time=   3.5s\n",
      "[CV 1/5; 124/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=16, neuron2=2\n",
      "[CV 1/5; 124/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=16, neuron2=2; total time=   3.5s\n",
      "[CV 2/5; 124/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=16, neuron2=2\n",
      "[CV 2/5; 124/8748] END activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=16, neuron2=2; total time=   3.5s\n",
      "[CV 3/5; 124/8748] START activation_function=softmax, batch_size=10, dropout_rate=0.7, epochs=50, init=normal, learning_rate=0.01, neuron1=16, neuron2=2\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters all at once and process will take more time\n",
    "def create_model(learning_rate,dropout_rate,activation_function,init,neuron1,neuron2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neuron1,input_dim = 28,kernel_initializer = init,activation = activation_function))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(neuron2,input_dim = neuron1,kernel_initializer = init,activation = activation_function))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1,activation = 'sigmoid'))\n",
    "    \n",
    "    adam = Adam(lr = learning_rate)\n",
    "    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model,verbose = 0)\n",
    "\n",
    "# Define the grid search parameters\n",
    "\n",
    "batch_size = [10,20,40]\n",
    "epochs = [10,50,100]\n",
    "learning_rate = [0.001,0.01,0.1]\n",
    "dropout_rate = [0.7,0.5,0.3]\n",
    "activation_function = ['softmax','relu','tanh','linear']\n",
    "init = ['uniform','normal','zero']\n",
    "neuron1 = [4,8,16]\n",
    "neuron2 = [2,4,8]\n",
    "\n",
    "# Make a dictionary of the grid search parameters\n",
    "\n",
    "param_grids = dict(batch_size = batch_size,epochs = epochs,learning_rate = learning_rate,dropout_rate = dropout_rate,\n",
    "                   activation_function = activation_function,init = init,neuron1 = neuron1,neuron2 = neuron2)\n",
    "\n",
    "# Build and fit the GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(estimator = model,param_grid = param_grids,cv = KFold(),verbose = 10)\n",
    "grid_result = grid.fit(X_standardized,Y)\n",
    "\n",
    "# Summarize the results\n",
    "print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print('{},{} with: {}'.format(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
